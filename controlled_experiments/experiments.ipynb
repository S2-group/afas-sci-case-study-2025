{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Reduction Experiment Analysis\n",
    "\n",
    "**Empirical Assessment of Carbon Reduction Strategies in Enterprise Software Applications**\n",
    "\n",
    "This notebook presents the statistical analysis of controlled experiments investigating the impact of application-level configurations on carbon emissions in AFAS SB. The analysis addresses Research Questions 2 and 3 from the thesis, employing statistical methods to evaluate configuration impacts and optimization potential.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "**RQ2**: How do application-level configuration settings affect the rate of carbon emissions of AFAS SB?\n",
    "\n",
    "**RQ3**: To what extent can the rate of carbon emissions of AFAS SB be reduced by _combining_ application-level configurations while maintaining system performance and functionality?\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "**H1.0**: Application-level configuration changes do not significantly affect the SCI score  \n",
    "**H1.a**: At least one configuration change significantly affects the SCI score  \n",
    "\n",
    "**H2.0**: The combined optimal SCI score configuration does not significantly affect the SCI score  \n",
    "**H2.a**: The combined optimal SCI score configuration significantly affects the SCI score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "Loading experimental data from the controlled environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, levene, mannwhitneyu, kruskal, ttest_ind, f_oneway\n",
    "from scipy.stats import pearsonr, spearmanr, bootstrap\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import ttest_power\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.power import tt_solve_power\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "from matplotlib.patches import Rectangle\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.figsize': (10, 6),\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "figures_dir = Path('figures')\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "results_dir = Path('results_data')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Analysis environment initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI Calculator and Data Loading Functions\n",
    "\n",
    "Implementing the Software Carbon Intensity calculations according to Green Software Foundation specification and data loading utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCICalculator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.GRID_CARBON_INTENSITY = 272  # gCO2eq/kWh, April 2025, The Netherlands\n",
    "        self.TOTAL_EMBODIED_EMISSIONS = 1449.84  # kg CO2eq\n",
    "        self.HARDWARE_LIFESPAN_HOURS = 4 * 365 * 24  # 4 years\n",
    "        self.VM_CPU_CORES = 8\n",
    "        self.TOTAL_SERVER_CORES = 24\n",
    "        \n",
    "    def calculate_sci(self, power_watts: float, duration_seconds: float, \n",
    "                     functional_units: int) -> Dict[str, float]:\n",
    "        \n",
    "        # Operational Emissions (O = E × I)\n",
    "        energy_kwh = (power_watts * duration_seconds) / (1000 * 3600)\n",
    "        operational_emissions = energy_kwh * self.GRID_CARBON_INTENSITY\n",
    "        \n",
    "        # Embodied Emissions (M = TE × (TiR/EL) × (RR/ToR))\n",
    "        time_reserved_hours = duration_seconds / 3600\n",
    "        resource_ratio = self.VM_CPU_CORES / self.TOTAL_SERVER_CORES\n",
    "        \n",
    "        embodied_emissions = (\n",
    "            self.TOTAL_EMBODIED_EMISSIONS * 1000 *\n",
    "            (time_reserved_hours / self.HARDWARE_LIFESPAN_HOURS) *\n",
    "            resource_ratio\n",
    "        )\n",
    "        \n",
    "        # SCI Score = (O + M) / R\n",
    "        total_emissions = operational_emissions + embodied_emissions\n",
    "        sci_score = total_emissions / functional_units if functional_units > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'energy_kwh': energy_kwh,\n",
    "            'operational_emissions': operational_emissions,\n",
    "            'embodied_emissions': embodied_emissions,\n",
    "            'total_emissions': total_emissions,\n",
    "            'sci_score': sci_score\n",
    "        }\n",
    "\n",
    "def load_experiment_data(base_path: str = \"experiment_runner/afas-sb/experiments\") -> Dict[str, pd.DataFrame]:\n",
    "    sci_calc = SCICalculator()\n",
    "    data = {}\n",
    "    \n",
    "    experiments = {\n",
    "        'P1_E1': {'name': 'Baseline Configuration', 'category': 'baseline'},\n",
    "        'P2_E1': {'name': 'Parallelism Configuration', 'category': 'parallelism', 'factor': 'parallelism_config'},\n",
    "        'P2_E2': {'name': 'Logging Configuration', 'category': 'logging', 'factor': 'logging_config'},\n",
    "        'P2_E3': {'name': 'Cache Configuration', 'category': 'caching', 'factor': 'cache_config'},\n",
    "        'P2_E4': {'name': 'Compression Configuration', 'category': 'compression', 'factor': 'compression_config'},\n",
    "        'P2_E5': {'name': 'Garbage Collection Configuration', 'category': 'gc', 'factor': 'gc_config'},\n",
    "        'P3_E1': {'name': 'Carbon-Optimized Configuration', 'category': 'optimized', 'factor': 'optimization_config'}\n",
    "    }\n",
    "    \n",
    "    for exp_id in experiments.keys():\n",
    "        run_table_path = Path(base_path) / exp_id / 'run_table.csv'\n",
    "        \n",
    "        if run_table_path.exists():\n",
    "            df = pd.read_csv(run_table_path)\n",
    "            df = df[df['__done'] == 'DONE'].copy()\n",
    "            \n",
    "            if len(df) > 0:\n",
    "                sci_results = df.apply(lambda row: sci_calc.calculate_sci(\n",
    "                    power_watts=row.get('powerjoular_power', 0),\n",
    "                    duration_seconds=row.get('test_duration', 0),\n",
    "                    functional_units=row.get('scenario_count', 1)\n",
    "                ), axis=1)\n",
    "                \n",
    "                for key in ['energy_kwh', 'operational_emissions', 'embodied_emissions', 'total_emissions', 'sci_score']:\n",
    "                    df[key] = [result[key] for result in sci_results]\n",
    "                \n",
    "                df['experiment_id'] = exp_id\n",
    "                df['experiment_name'] = experiments[exp_id]['name']\n",
    "                df['category'] = experiments[exp_id]['category']\n",
    "                \n",
    "                if exp_id == 'P3_E1' and 'optimization_config' in df.columns:\n",
    "                    df = df[df['optimization_config'] == 'carbon_optimized'].copy()\n",
    "                \n",
    "                data[exp_id] = df\n",
    "                print(f\"Loaded {exp_id}: {len(df)} runs, mean SCI: {df['sci_score'].mean():.3f} gCO2eq/scenario\")\n",
    "    \n",
    "    return data, experiments\n",
    "\n",
    "experiment_data, experiments = load_experiment_data()\n",
    "all_data = pd.concat(experiment_data.values(), ignore_index=True)\n",
    "print(f\"\\nTotal runs loaded: {len(all_data)}\")\n",
    "print(f\"Overall mean SCI: {all_data['sci_score'].mean():.3f} gCO2eq/scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Duration Analysis\n",
    "\n",
    "Analyzing the execution time characteristics of all experimental runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_duration_statistics(experiment_data):\n",
    "    all_durations = []\n",
    "    duration_by_experiment = {}\n",
    "    \n",
    "    for exp_id, data in experiment_data.items():\n",
    "        if 'test_duration' in data.columns:\n",
    "            exp_durations = data['test_duration'].dropna()\n",
    "            all_durations.extend(exp_durations.tolist())\n",
    "            \n",
    "            duration_by_experiment[exp_id] = {\n",
    "                'count': len(exp_durations),\n",
    "                'mean': exp_durations.mean(),\n",
    "                'std': exp_durations.std(),\n",
    "                'median': exp_durations.median(),\n",
    "                'min': exp_durations.min(),\n",
    "                'max': exp_durations.max(),\n",
    "                'total': exp_durations.sum()\n",
    "            }\n",
    "    \n",
    "    all_durations = pd.Series(all_durations)\n",
    "    \n",
    "    duration_stats = {\n",
    "        'total_runs': len(all_durations),\n",
    "        'total_duration_seconds': all_durations.sum(),\n",
    "        'total_duration_minutes': all_durations.sum() / 60,\n",
    "        'total_duration_hours': all_durations.sum() / 3600,\n",
    "        'mean_duration': all_durations.mean(),\n",
    "        'std_duration': all_durations.std(),\n",
    "        'median_duration': all_durations.median(),\n",
    "        'min_duration': all_durations.min(),\n",
    "        'max_duration': all_durations.max(),\n",
    "        'percentile_25': all_durations.quantile(0.25),\n",
    "        'percentile_75': all_durations.quantile(0.75),\n",
    "        'by_experiment': duration_by_experiment\n",
    "    }\n",
    "    \n",
    "    return duration_stats\n",
    "\n",
    "duration_stats = calculate_duration_statistics(experiment_data)\n",
    "\n",
    "print(\"=== RUN DURATION ANALYSIS ===\\n\")\n",
    "\n",
    "print(f\"Total number of runs: {duration_stats['total_runs']}\")\n",
    "print(f\"Total duration of all runs: {duration_stats['total_duration_seconds']:.1f} seconds\")\n",
    "print(f\"Total duration of all runs: {duration_stats['total_duration_minutes']:.1f} minutes\")\n",
    "print(f\"Total duration of all runs: {duration_stats['total_duration_hours']:.2f} hours\")\n",
    "print(f\"\\nMean run duration: {duration_stats['mean_duration']:.1f} seconds\")\n",
    "print(f\"Median run duration: {duration_stats['median_duration']:.1f} seconds\")\n",
    "print(f\"Standard deviation: {duration_stats['std_duration']:.1f} seconds\")\n",
    "print(f\"Min run duration: {duration_stats['min_duration']:.1f} seconds\")\n",
    "print(f\"Max run duration: {duration_stats['max_duration']:.1f} seconds\")\n",
    "print(f\"25th percentile: {duration_stats['percentile_25']:.1f} seconds\")\n",
    "print(f\"75th percentile: {duration_stats['percentile_75']:.1f} seconds\")\n",
    "\n",
    "print(f\"\\n=== DURATION BY EXPERIMENT ===\")\n",
    "for exp_id, stats in duration_stats['by_experiment'].items():\n",
    "    print(f\"\\n{exp_id}:\")\n",
    "    print(f\"  Runs: {stats['count']}\")\n",
    "    print(f\"  Total duration: {stats['total']:.1f} seconds ({stats['total']/60:.1f} minutes)\")\n",
    "    print(f\"  Mean: {stats['mean']:.1f} ± {stats['std']:.1f} seconds\")\n",
    "    print(f\"  Range: {stats['min']:.1f} - {stats['max']:.1f} seconds\")\n",
    "\n",
    "duration_summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': exp_id,\n",
    "        'Run_Count': stats['count'],\n",
    "        'Total_Duration_Minutes': stats['total'] / 60,\n",
    "        'Mean_Duration_Seconds': stats['mean'],\n",
    "        'Std_Duration_Seconds': stats['std'],\n",
    "        'Min_Duration_Seconds': stats['min'],\n",
    "        'Max_Duration_Seconds': stats['max']\n",
    "    }\n",
    "    for exp_id, stats in duration_stats['by_experiment'].items()\n",
    "])\n",
    "\n",
    "print(f\"\\n=== DURATION SUMMARY TABLE ===\")\n",
    "print(duration_summary_df.round(1).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Test Scenario Analysis\n",
    "\n",
    "Analyzing the characteristics of individual test scenarios within each experimental run to understand the granular workload composition and execution patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_stats():\n",
    "    print(\"=== TEST SCENARIO ANALYSIS ===\\n\")\n",
    "    \n",
    "    import glob\n",
    "    scenario_files = glob.glob(\"experiment_runner/afas-sb/experiments/*/*/scenario_summary.csv\")\n",
    "    \n",
    "    if scenario_files:\n",
    "        all_scenarios = []\n",
    "        valid_files = 0\n",
    "        \n",
    "        for file_path in scenario_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if not df.empty:\n",
    "                    all_scenarios.append(df)\n",
    "                    valid_files += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if all_scenarios:\n",
    "            combined_scenarios = pd.concat(all_scenarios, ignore_index=True)\n",
    "            \n",
    "            print(f\"Analysis across ALL experimental runs:\")\n",
    "            print(f\"Total scenario files analyzed: {valid_files}\")\n",
    "            print(f\"Total individual scenario executions: {len(combined_scenarios)}\")\n",
    "            print(f\"Scenarios per run: {len(combined_scenarios) // valid_files}\")\n",
    "            \n",
    "            if 'duration' in combined_scenarios.columns:\n",
    "                durations = combined_scenarios['duration'].dropna() / 1000\n",
    "                print(f\"\\nScenario Duration Statistics:\")\n",
    "                print(f\"  Mean duration: {durations.mean():.1f} seconds\")\n",
    "                print(f\"  Median duration: {durations.median():.1f} seconds\")\n",
    "                print(f\"  Standard deviation: {durations.std():.1f} seconds\")\n",
    "                print(f\"  Duration range: {durations.min():.1f} - {durations.max():.1f} seconds\")\n",
    "                print(f\"  25th percentile: {durations.quantile(0.25):.1f} seconds\")\n",
    "                print(f\"  75th percentile: {durations.quantile(0.75):.1f} seconds\")\n",
    "            \n",
    "            if 'status' in combined_scenarios.columns:\n",
    "                success_rate = (combined_scenarios['status'] == 'Succeeded').mean() * 100\n",
    "                failed_scenarios = len(combined_scenarios[combined_scenarios['status'] != 'Succeeded'])\n",
    "                print(f\"\\nReliability Statistics:\")\n",
    "                print(f\"  Overall success rate: {success_rate:.1f}%\")\n",
    "                print(f\"  Failed scenarios: {failed_scenarios} out of {len(combined_scenarios)}\")\n",
    "        else:\n",
    "            print(\"No valid scenario data could be loaded\")\n",
    "    else:\n",
    "        print(\"No scenario summary files found\")\n",
    "\n",
    "get_scenario_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Baseline Reference\n",
    "\n",
    "Establishing the baseline reference (P1_E1) for comparative analysis and preparing configuration-specific datasets for RQ2 analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = experiment_data['P1_E1']\n",
    "baseline_sci_mean = baseline_data['sci_score'].mean()\n",
    "baseline_sci_std = baseline_data['sci_score'].std()\n",
    "\n",
    "print(f\"Baseline SCI: {baseline_sci_mean:.3f} ± {baseline_sci_std:.3f} gCO2eq/scenario (n={len(baseline_data)})\")\n",
    "\n",
    "config_experiments = {\n",
    "    'P2_E1': 'parallelism_config',\n",
    "    'P2_E2': 'logging_config',\n",
    "    'P2_E3': 'cache_config', \n",
    "    'P2_E5': 'gc_config'\n",
    "}\n",
    "\n",
    "config_data = {}\n",
    "for exp_id, config_column in config_experiments.items():\n",
    "    if exp_id in experiment_data and config_column in experiment_data[exp_id].columns:\n",
    "        exp_df = experiment_data[exp_id]\n",
    "        configs = exp_df[config_column].unique()\n",
    "        \n",
    "        for config in configs:\n",
    "            config_df = exp_df[exp_df[config_column] == config].copy()\n",
    "            config_key = f\"{exp_id}_{config}\"\n",
    "            config_data[config_key] = config_df\n",
    "            print(f\"{config_key}: {len(config_df)} runs\")\n",
    "\n",
    "if 'P2_E4' in experiment_data:\n",
    "    config_data['P2_E4_no_compression'] = experiment_data['P2_E4']\n",
    "    print(f\"P2_E4_no_compression: {len(experiment_data['P2_E4'])} runs\")\n",
    "    \n",
    "if 'P3_E1' in experiment_data:\n",
    "    config_data['P3_E1_carbon_optimized'] = experiment_data['P3_E1']\n",
    "    print(f\"P3_E1_carbon_optimized: {len(experiment_data['P3_E1'])} runs\")\n",
    "\n",
    "print(f\"\\nTotal configurations for analysis: {len(config_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Comparison Framework\n",
    "\n",
    "Implementing statistical comparison functions to analyze configuration effects against baseline with appropriate test selection based on data distribution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_comparison(config_data: pd.DataFrame, baseline_data: pd.DataFrame, \n",
    "                         metric: str = 'sci_score') -> Dict:\n",
    "    config_values = config_data[metric].dropna()\n",
    "    baseline_values = baseline_data[metric].dropna()\n",
    "    \n",
    "    if len(config_values) < 3 or len(baseline_values) < 3:\n",
    "        return {'error': 'Insufficient data'}\n",
    "    \n",
    "    config_normal = shapiro(config_values)[1] > 0.05\n",
    "    baseline_normal = shapiro(baseline_values)[1] > 0.05\n",
    "    \n",
    "    if config_normal and baseline_normal:\n",
    "        equal_var = levene(config_values, baseline_values)[1] > 0.05\n",
    "        stat, p_value = ttest_ind(config_values, baseline_values, equal_var=equal_var)\n",
    "        test_type = f\"t-test ({'equal' if equal_var else 'unequal'} var)\"\n",
    "        \n",
    "        pooled_std = np.sqrt(((len(config_values)-1)*config_values.var() + \n",
    "                             (len(baseline_values)-1)*baseline_values.var()) / \n",
    "                            (len(config_values)+len(baseline_values)-2))\n",
    "        effect_size = (baseline_values.mean() - config_values.mean()) / pooled_std\n",
    "    else:\n",
    "        stat, p_value = mannwhitneyu(config_values, baseline_values, alternative='two-sided')\n",
    "        test_type = \"Mann-Whitney U\"\n",
    "        n1, n2 = len(baseline_values), len(config_values)\n",
    "        cliff_delta = (sum(x > y for x in baseline_values for y in config_values) - \n",
    "                      sum(x < y for x in baseline_values for y in config_values)) / (n1 * n2)\n",
    "        effect_size = cliff_delta\n",
    "    \n",
    "    improvement = ((baseline_values.mean() - config_values.mean()) / \n",
    "                  baseline_values.mean()) * 100\n",
    "    \n",
    "    return {\n",
    "        'config_mean': config_values.mean(),\n",
    "        'config_std': config_values.std(),\n",
    "        'baseline_mean': baseline_values.mean(),\n",
    "        'baseline_std': baseline_values.std(),\n",
    "        'improvement_pct': improvement,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': effect_size,\n",
    "        'test_type': test_type,\n",
    "        'significant': p_value < 0.05,\n",
    "        'config_n': len(config_values),\n",
    "        'baseline_n': len(baseline_values)\n",
    "    }\n",
    "\n",
    "rq2_results = {}\n",
    "metrics = ['sci_score', 'powerjoular_power', 'test_duration', 'powerjoular_util']\n",
    "\n",
    "for config_name, config_df in config_data.items():\n",
    "    rq2_results[config_name] = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in config_df.columns and metric in baseline_data.columns:\n",
    "            result = statistical_comparison(config_df, baseline_data, metric)\n",
    "            rq2_results[config_name][metric] = result\n",
    "\n",
    "print(\"Statistical analysis completed for all configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Comparison Correction\n",
    "\n",
    "Implementing Bonferroni and Holm corrections for multiple comparisons to maintain statistical rigor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_for_correction = []\n",
    "test_descriptions = []\n",
    "\n",
    "for config_name, metrics_data in rq2_results.items():\n",
    "    for metric, result in metrics_data.items():\n",
    "        if not result.get('error') and 'p_value' in result:\n",
    "            p_values_for_correction.append(result['p_value'])\n",
    "            test_descriptions.append(f\"{config_name}_{metric}\")\n",
    "\n",
    "if p_values_for_correction:\n",
    "    bonferroni_rejected, bonferroni_corrected, _, bonferroni_alpha = multipletests(\n",
    "        p_values_for_correction, alpha=0.05, method='bonferroni'\n",
    "    )\n",
    "    \n",
    "    holm_rejected, holm_corrected, _, holm_alpha = multipletests(\n",
    "        p_values_for_correction, alpha=0.05, method='holm'\n",
    "    )\n",
    "    \n",
    "    correction_results = pd.DataFrame({\n",
    "        'Test': test_descriptions,\n",
    "        'Original_P_Value': p_values_for_correction,\n",
    "        'Bonferroni_Corrected_P': bonferroni_corrected,\n",
    "        'Bonferroni_Significant': bonferroni_rejected,\n",
    "        'Holm_Corrected_P': holm_corrected,\n",
    "        'Holm_Significant': holm_rejected\n",
    "    })\n",
    "    \n",
    "    correction_results.to_csv(results_dir / 'multiple_comparison_corrections.csv', index=False)\n",
    "    \n",
    "    print(f\"Multiple Comparison Correction Applied:\")\n",
    "    print(f\"Total tests: {len(p_values_for_correction)}\")\n",
    "    print(f\"Original alpha: 0.05\")\n",
    "    print(f\"Bonferroni alpha: {bonferroni_alpha:.6f}\")\n",
    "    print(f\"Significant after Bonferroni: {sum(bonferroni_rejected)}\")\n",
    "    print(f\"Significant after Holm: {sum(holm_rejected)}\")\n",
    "    \n",
    "    significant_after_correction = correction_results[\n",
    "        correction_results['Holm_Significant'] == True\n",
    "    ].sort_values('Holm_Corrected_P')\n",
    "    \n",
    "    if len(significant_after_correction) > 0:\n",
    "        print(f\"\\nSignificant results after Holm correction:\")\n",
    "        for _, row in significant_after_correction.head(10).iterrows():\n",
    "            print(f\"  {row['Test']}: p = {row['Holm_Corrected_P']:.6f}\")\n",
    "    else:\n",
    "        print(\"\\nNo results remain significant after multiple comparison correction.\")\n",
    "else:\n",
    "    print(\"No p-values available for multiple comparison correction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Confidence Intervals\n",
    "\n",
    "Using bootstrap resampling to estimate confidence intervals for effect sizes and validate the robustness of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_effect_size(group1, group2, n_bootstrap=1000, confidence_level=0.95):\n",
    "\n",
    "    def cohen_d(x, y):\n",
    "        \"\"\"Calculate Cohen's d effect size\"\"\"\n",
    "        pooled_std = np.sqrt(((len(x) - 1) * np.var(x, ddof=1) + \n",
    "                             (len(y) - 1) * np.var(y, ddof=1)) / \n",
    "                            (len(x) + len(y) - 2))\n",
    "        return (np.mean(x) - np.mean(y)) / pooled_std\n",
    "    \n",
    "    bootstrap_effects = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_group1 = np.random.choice(group1, size=len(group1), replace=True)\n",
    "        boot_group2 = np.random.choice(group2, size=len(group2), replace=True)\n",
    "        \n",
    "        effect = cohen_d(boot_group1, boot_group2)\n",
    "        bootstrap_effects.append(effect)\n",
    "    \n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_effects, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_effects, upper_percentile)\n",
    "    \n",
    "    return {\n",
    "        'effect_size': cohen_d(group1, group2),\n",
    "        'bootstrap_mean': np.mean(bootstrap_effects),\n",
    "        'bootstrap_std': np.std(bootstrap_effects),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'bootstrap_effects': bootstrap_effects\n",
    "    }\n",
    "\n",
    "bootstrap_results = []\n",
    "\n",
    "for config_name, config_df in config_data.items():\n",
    "    if 'sci_score' in config_df.columns:\n",
    "        config_sci = config_df['sci_score'].dropna()\n",
    "        baseline_sci = baseline_data['sci_score'].dropna()\n",
    "        \n",
    "        if len(config_sci) >= 3 and len(baseline_sci) >= 3:\n",
    "            boot_result = bootstrap_effect_size(baseline_sci, config_sci)\n",
    "            \n",
    "            ci_significant = not (boot_result['ci_lower'] <= 0 <= boot_result['ci_upper'])\n",
    "            \n",
    "            bootstrap_results.append({\n",
    "                'Configuration': config_name,\n",
    "                'Effect_Size': boot_result['effect_size'],\n",
    "                'Bootstrap_Mean': boot_result['bootstrap_mean'],\n",
    "                'Bootstrap_Std': boot_result['bootstrap_std'],\n",
    "                'CI_Lower': boot_result['ci_lower'],\n",
    "                'CI_Upper': boot_result['ci_upper'],\n",
    "                'CI_Significant': ci_significant\n",
    "            })\n",
    "\n",
    "bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "if not bootstrap_df.empty:\n",
    "    bootstrap_df = bootstrap_df.sort_values('Effect_Size', ascending=False)\n",
    "    bootstrap_df.to_csv(results_dir / 'bootstrap_effect_sizes.csv', index=False)\n",
    "    \n",
    "    print(\"Bootstrap Effect Size Analysis (SCI Score):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for _, row in bootstrap_df.head(10).iterrows():\n",
    "        significance = \"***\" if row['CI_Significant'] else \"n.s.\"\n",
    "        print(f\"{row['Configuration'][:25]:25} Effect: {row['Effect_Size']:6.3f} \"\n",
    "              f\"95% CI: [{row['CI_Lower']:6.3f}, {row['CI_Upper']:6.3f}] {significance}\")\n",
    "    \n",
    "    robust_significant = bootstrap_df[bootstrap_df['CI_Significant'] == True]\n",
    "    print(f\"\\nConfigurations with robust significant effects: {len(robust_significant)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No bootstrap results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis and Sample Size Assessment\n",
    "\n",
    "Conducting post-hoc power analysis to assess the adequacy of sample sizes and identify potential Type II errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_analysis_results = []\n",
    "\n",
    "for exp_id, exp_df in experiment_data.items():\n",
    "    if exp_id != 'P1_E1':\n",
    "        exp_sci = exp_df['sci_score'].dropna()\n",
    "        baseline_sci = baseline_data['sci_score'].dropna()\n",
    "        \n",
    "        if len(exp_sci) >= 3 and len(baseline_sci) >= 3:\n",
    "            pooled_std = np.sqrt(((len(exp_sci) - 1) * exp_sci.var() + \n",
    "                                 (len(baseline_sci) - 1) * baseline_sci.var()) / \n",
    "                                (len(exp_sci) + len(baseline_sci) - 2))\n",
    "            \n",
    "            observed_effect = abs((baseline_sci.mean() - exp_sci.mean()) / pooled_std)\n",
    "            \n",
    "            achieved_power = ttest_power(\n",
    "                effect_size=observed_effect,\n",
    "                nobs=min(len(exp_sci), len(baseline_sci)),\n",
    "                alpha=0.05,\n",
    "                alternative='two-sided'\n",
    "            )\n",
    "            \n",
    "            required_n_80 = int(np.ceil(tt_solve_power(\n",
    "                effect_size=observed_effect,\n",
    "                power=0.8,\n",
    "                alpha=0.05,\n",
    "                alternative='two-sided'\n",
    "            ))) if observed_effect > 0 else np.inf\n",
    "            \n",
    "            required_n_90 = int(np.ceil(tt_solve_power(\n",
    "                effect_size=observed_effect,\n",
    "                power=0.9,\n",
    "                alpha=0.05,\n",
    "                alternative='two-sided'\n",
    "            ))) if observed_effect > 0 else np.inf\n",
    "            \n",
    "            power_analysis_results.append({\n",
    "                'Experiment': exp_id,\n",
    "                'Sample_Size': len(exp_sci),\n",
    "                'Observed_Effect_Size': observed_effect,\n",
    "                'Achieved_Power': achieved_power,\n",
    "                'Required_N_80_Power': required_n_80,\n",
    "                'Required_N_90_Power': required_n_90,\n",
    "                'Adequate_Power_80': achieved_power >= 0.8,\n",
    "                'Adequate_Power_90': achieved_power >= 0.9\n",
    "            })\n",
    "\n",
    "power_df = pd.DataFrame(power_analysis_results)\n",
    "if not power_df.empty:\n",
    "    power_df.to_csv(results_dir / 'power_analysis_results.csv', index=False)\n",
    "    \n",
    "    print(\"Post-hoc Power Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Experiment':<12} {'N':<4} {'Effect':<8} {'Power':<8} {'80% Power':<11} {'90% Power':<11}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for _, row in power_df.iterrows():\n",
    "        power_80_status = \"✓\" if row['Adequate_Power_80'] else \"✗\"\n",
    "        power_90_status = \"✓\" if row['Adequate_Power_90'] else \"✗\"\n",
    "        \n",
    "        print(f\"{row['Experiment']:<12} {row['Sample_Size']:<4} \"\n",
    "              f\"{row['Observed_Effect_Size']:<8.3f} {row['Achieved_Power']:<8.3f} \"\n",
    "              f\"{power_80_status:<11} {power_90_status:<11}\")\n",
    "    \n",
    "    adequate_80 = power_df['Adequate_Power_80'].sum()\n",
    "    adequate_90 = power_df['Adequate_Power_90'].sum()\n",
    "    total_exp = len(power_df)\n",
    "    \n",
    "    print(f\"\\nPower Analysis Summary:\")\n",
    "    print(f\"Experiments with adequate power (≥80%): {adequate_80}/{total_exp} ({adequate_80/total_exp*100:.1f}%)\")\n",
    "    print(f\"Experiments with high power (≥90%): {adequate_90}/{total_exp} ({adequate_90/total_exp*100:.1f}%)\")\n",
    "    \n",
    "    low_power = power_df[power_df['Achieved_Power'] < 0.8]\n",
    "    if len(low_power) > 0:\n",
    "        print(f\"\\nExperiments with potentially inadequate power (risk of Type II error):\")\n",
    "        for _, row in low_power.iterrows():\n",
    "            print(f\"   {row['Experiment']}: Power = {row['Achieved_Power']:.3f}, \"\n",
    "                  f\"Recommend n ≥ {row['Required_N_80_Power']} for 80% power\")\n",
    "else:\n",
    "    print(\"No power analysis results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Application-Level Configuration Settings\n",
    "\n",
    "Analyzing how different configuration parameters affect carbon emissions compared to the baseline (P1_E1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline SCI: {baseline_sci_mean:.3f} ± {baseline_sci_std:.3f} gCO2eq/scenario (n={len(baseline_data)})\")\n",
    "print(\"\\nRQ2 Analysis: Comparing configurations against baseline\\n\")\n",
    "\n",
    "rq2_comparison_results = []\n",
    "\n",
    "for exp_id in ['P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5']:\n",
    "    if exp_id in experiment_data:\n",
    "        exp_data = experiment_data[exp_id]\n",
    "        factor_col = experiments[exp_id].get('factor')\n",
    "        \n",
    "        print(f\"\\n{experiments[exp_id]['name']} ({exp_id}):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if factor_col and factor_col in exp_data.columns:\n",
    "            configurations = exp_data[factor_col].unique()\n",
    "            \n",
    "            for config in configurations:\n",
    "                config_data_subset = exp_data[exp_data[factor_col] == config]\n",
    "                config_sci_mean = config_data_subset['sci_score'].mean()\n",
    "                config_sci_std = config_data_subset['sci_score'].std()\n",
    "                \n",
    "                improvement = ((baseline_sci_mean - config_sci_mean) / baseline_sci_mean) * 100\n",
    "                \n",
    "                stat, p_value = mannwhitneyu(baseline_data['sci_score'], config_data_subset['sci_score'], alternative='two-sided')\n",
    "                \n",
    "                n1, n2 = len(baseline_data), len(config_data_subset)\n",
    "                cliff_delta = (sum(x > y for x in baseline_data['sci_score'] for y in config_data_subset['sci_score']) - \n",
    "                             sum(x < y for x in baseline_data['sci_score'] for y in config_data_subset['sci_score'])) / (n1 * n2)\n",
    "                \n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"n.s.\"\n",
    "                \n",
    "                print(f\"  {config}: {config_sci_mean:.3f} ± {config_sci_std:.3f} (n={len(config_data_subset)})\")\n",
    "                print(f\"    Improvement: {improvement:+.1f}% | p={p_value:.4f} {significance} | δ={cliff_delta:.3f}\")\n",
    "                \n",
    "                rq2_comparison_results.append({\n",
    "                    'experiment': exp_id,\n",
    "                    'experiment_name': experiments[exp_id]['name'],\n",
    "                    'configuration': config,\n",
    "                    'baseline_mean': baseline_sci_mean,\n",
    "                    'config_mean': config_sci_mean,\n",
    "                    'improvement_percent': improvement,\n",
    "                    'p_value': p_value,\n",
    "                    'cliff_delta': cliff_delta,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'n_baseline': len(baseline_data),\n",
    "                    'n_config': len(config_data_subset)\n",
    "                })\n",
    "        else:\n",
    "            exp_sci_mean = exp_data['sci_score'].mean()\n",
    "            exp_sci_std = exp_data['sci_score'].std()\n",
    "            improvement = ((baseline_sci_mean - exp_sci_mean) / baseline_sci_mean) * 100\n",
    "            \n",
    "            stat, p_value = mannwhitneyu(baseline_data['sci_score'], exp_data['sci_score'], alternative='two-sided')\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"n.s.\"\n",
    "            print(f\"  Overall: {exp_sci_mean:.3f} ± {exp_sci_std:.3f} (n={len(exp_data)})\")\n",
    "            print(f\"    Improvement: {improvement:+.1f}% | p={p_value:.4f} {significance}\")\n",
    "\n",
    "rq2_df = pd.DataFrame(rq2_comparison_results)\n",
    "if not rq2_df.empty:\n",
    "    rq2_df.to_csv(results_dir / 'rq2_statistical_analysis.csv', index=False)\n",
    "\n",
    "    significant_improvements = rq2_df[rq2_df['significant'] & (rq2_df['improvement_percent'] > 0)]\n",
    "    print(f\"\\nRQ2 Summary: {len(significant_improvements)} out of {len(rq2_df)} configurations show significant improvement\")\n",
    "    if len(rq2_df) > 0:\n",
    "        print(f\"Best improvement: {rq2_df['improvement_percent'].max():.1f}% (p={rq2_df.loc[rq2_df['improvement_percent'].idxmax(), 'p_value']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis Results\n",
    "\n",
    "Analyzing significance and effect sizes across all configuration metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_results = []\n",
    "\n",
    "for config_name, metrics_data in rq2_results.items():\n",
    "    sci_result = metrics_data.get('sci_score', {})\n",
    "    if sci_result.get('significant', False) and not sci_result.get('error'):\n",
    "        significant_results.append({\n",
    "            'Configuration': config_name,\n",
    "            'SCI_Mean': sci_result['config_mean'],\n",
    "            'SCI_Std': sci_result['config_std'],\n",
    "            'Improvement_%': sci_result['improvement_pct'],\n",
    "            'P_Value': sci_result['p_value'],\n",
    "            'Effect_Size': sci_result['effect_size'],\n",
    "            'Test_Type': sci_result['test_type'],\n",
    "            'Sample_Size': sci_result['config_n']\n",
    "        })\n",
    "\n",
    "significant_df = pd.DataFrame(significant_results)\n",
    "if not significant_df.empty:\n",
    "    significant_df = significant_df.sort_values('Improvement_%', ascending=False)\n",
    "    significant_df.to_csv(results_dir / 'rq2_significant_configurations.csv', index=False)\n",
    "    \n",
    "    print(\"Significant SCI Improvements vs Baseline:\")\n",
    "    print(significant_df[['Configuration', 'Improvement_%', 'P_Value']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No statistically significant SCI improvements found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI Distribution Analysis by Configuration\n",
    "\n",
    "Visualizing the distribution of SCI scores across different configurations using violin plots to show central tendency and distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "\n",
    "for _, row in baseline_data.iterrows():\n",
    "    plot_data.append({\n",
    "        'SCI_Score': row['sci_score'],\n",
    "        'Configuration': 'Baseline',\n",
    "        'Experiment': 'P1_E1'\n",
    "    })\n",
    "\n",
    "for exp_id in ['P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5']:\n",
    "    if exp_id in experiment_data:\n",
    "        exp_data = experiment_data[exp_id]\n",
    "        factor_col = experiments[exp_id].get('factor')\n",
    "\n",
    "        if factor_col and factor_col in exp_data.columns:\n",
    "            for _, row in exp_data.iterrows():\n",
    "                plot_data.append({\n",
    "                    'SCI_Score': row['sci_score'],\n",
    "                    'Configuration': row[factor_col],\n",
    "                    'Experiment': exp_id\n",
    "                })\n",
    "\n",
    "if 'P3_E1' in experiment_data:\n",
    "    for _, row in experiment_data['P3_E1'].iterrows():\n",
    "        plot_data.append({\n",
    "            'SCI_Score': row['sci_score'],\n",
    "            'Configuration': 'Carbon-Optimized',\n",
    "            'Experiment': 'P3_E1'\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "experiment_order = ['P1_E1', 'P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5', 'P3_E1']\n",
    "\n",
    "config_to_experiment = plot_df.set_index('Configuration')['Experiment'].to_dict()\n",
    "\n",
    "unique_configs = plot_df['Configuration'].unique()\n",
    "config_order = sorted(unique_configs, key=lambda c: experiment_order.index(config_to_experiment[c]))\n",
    "\n",
    "experiment_palette = sns.color_palette('tab10', len(experiment_order))\n",
    "experiment_color_map = dict(zip(experiment_order, experiment_palette))\n",
    "\n",
    "violin_plot = sns.violinplot(\n",
    "    data=plot_df,\n",
    "    x='Configuration',\n",
    "    y='SCI_Score',\n",
    "    order=config_order,\n",
    "    inner='box',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(config_order, rotation=45)\n",
    "ax.set_xlabel('Configuration', fontsize=14)\n",
    "ax.set_ylabel('SCI Score (gCO₂e/scenario)', fontsize=14)\n",
    "\n",
    "ax.axhline(y=baseline_sci_mean, color='red', linestyle='--', alpha=0.7,\n",
    "           label=f'Baseline Mean: {baseline_sci_mean:.3f}')\n",
    "\n",
    "for i, config in enumerate(config_order):\n",
    "    experiment = config_to_experiment[config]\n",
    "    color = experiment_color_map[experiment]\n",
    "    violin_plot.collections[i * 2].set_facecolor(color)\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color, label=exp) for exp, color in experiment_color_map.items()]\n",
    "ax.legend(handles=legend_patches, title='Experiment', bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'sci_distribution_by_configuration.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Violin plot saved to {figures_dir / 'sci_distribution_by_configuration.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Effect Heatmap\n",
    "\n",
    "Creating a heatmap showing the effect sizes and significance levels of different configurations across multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = []\n",
    "metrics_display = {\n",
    "    'sci_score': 'SCI Score',\n",
    "    'powerjoular_power': 'Power (W)',\n",
    "    'test_duration': 'Duration (s)',\n",
    "    'powerjoular_util': 'CPU Usage (%)'\n",
    "}\n",
    "\n",
    "for config_name, metrics_data in rq2_results.items():\n",
    "    for metric, metric_display in metrics_display.items():\n",
    "        result = metrics_data.get(metric, {})\n",
    "        if not result.get('error'):\n",
    "            heatmap_data.append({\n",
    "                'Configuration': config_name.replace('_', ' '),\n",
    "                'Metric': metric_display,\n",
    "                'Improvement_%': result.get('improvement_pct', 0),\n",
    "                'Effect_Size': abs(result.get('effect_size', 0)),\n",
    "                'Significant': result.get('significant', False),\n",
    "                'P_Value': result.get('p_value', 1.0)\n",
    "            })\n",
    "\n",
    "heatmap_df = pd.DataFrame(heatmap_data)\n",
    "\n",
    "if not heatmap_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    pivot_improvement = heatmap_df.pivot(index='Configuration', columns='Metric', values='Improvement_%')\n",
    "    sns.heatmap(pivot_improvement, annot=True, fmt='.1f', cmap='RdYlGn', center=0,\n",
    "                ax=ax1, cbar_kws={'label': 'Improvement %'})\n",
    "    ax1.collections[0].colorbar.ax.set_ylabel('Improvement %', fontsize=14)\n",
    "    ax1.set_xlabel('Metrics', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('Configuration', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    pivot_sig = heatmap_df.pivot(index='Configuration', columns='Metric', values='Significant')\n",
    "    for i in range(len(pivot_improvement.index)):\n",
    "        for j in range(len(pivot_improvement.columns)):\n",
    "            if pivot_sig.iloc[i, j]:\n",
    "                ax1.add_patch(Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=3))\n",
    "    \n",
    "    pivot_effect = heatmap_df.pivot(index='Configuration', columns='Metric', values='Effect_Size')\n",
    "    sns.heatmap(pivot_effect, annot=True, fmt='.2f', cmap='viridis',\n",
    "                ax=ax2, cbar_kws={'label': 'Effect Size (absolute)'})\n",
    "    ax2.collections[0].colorbar.ax.set_ylabel('Effect Size (absolute)', fontsize=14)\n",
    "    ax2.set_xlabel('Metrics', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('')\n",
    "\n",
    "    ax1.tick_params(axis='x', labelsize=12)\n",
    "    ax1.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    ax2.tick_params(axis='x', labelsize=12)\n",
    "    ax2.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "    \n",
    "    for i in range(len(pivot_effect.index)):\n",
    "        for j in range(len(pivot_effect.columns)):\n",
    "            if pivot_sig.iloc[i, j]:\n",
    "                ax2.add_patch(Rectangle((j, i), 1, 1, fill=False, edgecolor='white', lw=3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'rq2_configuration_effects_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    heatmap_df.to_csv(results_dir / 'rq2_all_configuration_effects.csv', index=False)\n",
    "    print(\"Configuration effects heatmap saved\")\n",
    "else:\n",
    "    print(\"Insufficient data for heatmap visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Utilization vs SCI Analysis\n",
    "\n",
    "Examining the relationship between system resource utilization and carbon intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_data = []\n",
    "\n",
    "for exp_id, exp_df in experiment_data.items():\n",
    "    for _, row in exp_df.iterrows():\n",
    "        if pd.notna(row.get('vm_avg_cpu_percent')) and pd.notna(row.get('vm_total_io_read_count')):\n",
    "            resource_data.append({\n",
    "                'SCI_Score': row['sci_score'],\n",
    "                'CPU_Utilization': row['vm_avg_cpu_percent'],\n",
    "                'IO_Read_Operations': row['vm_total_io_read_count'] / 1e6,\n",
    "                'Experiment': exp_id,\n",
    "                'Category': experiments[exp_id]['category'],\n",
    "                'test_duration': row['test_duration']\n",
    "            })\n",
    "\n",
    "resource_df = pd.DataFrame(resource_data)\n",
    "\n",
    "if not resource_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    \n",
    "    scatter1 = ax1.scatter(\n",
    "        resource_df['CPU_Utilization'], resource_df['SCI_Score'],\n",
    "        c=resource_df['test_duration'], s=60, alpha=0.7,\n",
    "        cmap='plasma', edgecolors='black', linewidth=0.5\n",
    "    )\n",
    "    ax1.set_xlabel('CPU Utilization (%)', fontsize=12)\n",
    "    ax1.set_ylabel('SCI Score (gCO₂e/scenario)', fontsize=12)\n",
    "    \n",
    "    corr_cpu = resource_df['CPU_Utilization'].corr(resource_df['SCI_Score'])\n",
    "    ax1.text(0.05, 0.95, f'r = {corr_cpu:.3f}', transform=ax1.transAxes,\n",
    "             fontsize=12, fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    scatter2 = ax2.scatter(\n",
    "        resource_df['IO_Read_Operations'], resource_df['SCI_Score'],\n",
    "        c=resource_df['test_duration'], s=60, alpha=0.7,\n",
    "        cmap='plasma', edgecolors='black', linewidth=0.5\n",
    "    )\n",
    "    ax2.set_xlabel('I/O Read Operations (millions)', fontsize=12)\n",
    "    ax2.set_ylabel('SCI Score (gCO₂e/scenario)', fontsize=12)\n",
    "    \n",
    "    corr_io = resource_df['IO_Read_Operations'].corr(resource_df['SCI_Score'])\n",
    "    ax2.text(0.05, 0.95, f'r = {corr_io:.3f}', transform=ax2.transAxes,\n",
    "             fontsize=12, fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "    cbar = fig.colorbar(scatter1, cax=cbar_ax)\n",
    "    cbar.set_label('Duration (s)', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.savefig(figures_dir / 'resource_utilization_vs_sci.png')\n",
    "    plt.show()\n",
    "    \n",
    "    correlation_results = pd.DataFrame({\n",
    "        'Metric_Pair': ['SCI_vs_CPU', 'SCI_vs_IO_Read', 'Duration_vs_CPU', 'Duration_vs_IO_Read'],\n",
    "        'Correlation': [\n",
    "            corr_cpu,\n",
    "            corr_io,\n",
    "            resource_df['test_duration'].corr(resource_df['CPU_Utilization']),\n",
    "            resource_df['test_duration'].corr(resource_df['IO_Read_Operations'])\n",
    "        ]\n",
    "    })\n",
    "    correlation_results.to_csv(results_dir / 'resource_correlations.csv', index=False)\n",
    "    \n",
    "    print(f\"Resource utilization plot saved to {figures_dir / 'resource_utilization_vs_sci.png'}\")\n",
    "    print(f\"Correlation analysis saved to {results_dir / 'resource_correlations.csv'}\")\n",
    "else:\n",
    "    print(\"Insufficient resource utilization data for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Correlation Analysis\n",
    "\n",
    "Examining relationships between variables while controlling for potential confounders to identify causal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation(x, y, control_vars, data):\n",
    "    from scipy.stats import pearsonr\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    clean_data = data[[x, y] + control_vars].dropna()\n",
    "    \n",
    "    if len(clean_data) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    X_controls = clean_data[control_vars]\n",
    "    reg_x = LinearRegression().fit(X_controls, clean_data[x])\n",
    "    x_residuals = clean_data[x] - reg_x.predict(X_controls)\n",
    "    \n",
    "    reg_y = LinearRegression().fit(X_controls, clean_data[y])\n",
    "    y_residuals = clean_data[y] - reg_y.predict(X_controls)\n",
    "    \n",
    "    r, p_value = pearsonr(x_residuals, y_residuals)\n",
    "    \n",
    "    return r, p_value\n",
    "\n",
    "partial_correlations = []\n",
    "\n",
    "relationships = [\n",
    "    ('sci_score', 'powerjoular_power', ['test_duration']),\n",
    "    ('sci_score', 'powerjoular_util', ['test_duration']),\n",
    "    ('sci_score', 'vm_avg_memory_mb', ['test_duration', 'powerjoular_util']),\n",
    "    ('powerjoular_power', 'powerjoular_util', ['test_duration']),\n",
    "    ('test_duration', 'powerjoular_util', ['powerjoular_power'])\n",
    "]\n",
    "\n",
    "print(\"Partial Correlation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Relationship':<35} {'Zero-order r':<12} {'Partial r':<12} {'p-value':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for var1, var2, controls in relationships:\n",
    "    clean_data = all_data[[var1, var2]].dropna()\n",
    "    if len(clean_data) > 0:\n",
    "        zero_order_r, _ = pearsonr(clean_data[var1], clean_data[var2])\n",
    "        \n",
    "        partial_r, partial_p = partial_correlation(var1, var2, controls, all_data)\n",
    "        \n",
    "        if partial_r is not None:\n",
    "            relationship_name = f\"{var1} ↔ {var2}\"\n",
    "            controls_str = f\"(controlling for {', '.join(controls)})\"\n",
    "            \n",
    "            print(f\"{relationship_name:<35} {zero_order_r:<12.3f} {partial_r:<12.3f} {partial_p:<10.3f}\")\n",
    "            \n",
    "            partial_correlations.append({\n",
    "                'Variable_1': var1,\n",
    "                'Variable_2': var2,\n",
    "                'Control_Variables': ', '.join(controls),\n",
    "                'Zero_Order_r': zero_order_r,\n",
    "                'Partial_r': partial_r,\n",
    "                'P_Value': partial_p,\n",
    "                'Significant': partial_p < 0.05 if partial_p is not None else False\n",
    "            })\n",
    "\n",
    "if partial_correlations:\n",
    "    partial_corr_df = pd.DataFrame(partial_correlations)\n",
    "    partial_corr_df.to_csv(results_dir / 'partial_correlation_analysis.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nPartial correlation analysis saved to {results_dir / 'partial_correlation_analysis.csv'}\")\n",
    "    \n",
    "    print(\"\\nKey findings:\")\n",
    "    for _, row in partial_corr_df.iterrows():\n",
    "        diff = abs(row['Zero_Order_r'] - row['Partial_r'])\n",
    "        if diff > 0.2:\n",
    "            print(f\"  {row['Variable_1']} ↔ {row['Variable_2']}: \"\n",
    "                  f\"r changes from {row['Zero_Order_r']:.3f} to {row['Partial_r']:.3f} \"\n",
    "                  f\"when controlling for {row['Control_Variables']}\")\n",
    "else:\n",
    "    print(\"No partial correlation results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon-Optimized Configuration Effectiveness\n",
    "\n",
    "Evaluating the combined carbon-optimized configuration (P3_E1) against the baseline to test H3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'P3_E1' in experiment_data:\n",
    "    carbon_optimized_data = experiment_data['P3_E1']\n",
    "    \n",
    "    baseline_sci = baseline_data['sci_score']\n",
    "    optimized_sci = carbon_optimized_data['sci_score']\n",
    "    \n",
    "    baseline_mean = baseline_sci.mean()\n",
    "    baseline_std = baseline_sci.std()\n",
    "    optimized_mean = optimized_sci.mean()\n",
    "    optimized_std = optimized_sci.std()\n",
    "    \n",
    "    improvement = ((baseline_mean - optimized_mean) / baseline_mean) * 100\n",
    "    \n",
    "    stat, p_value = mannwhitneyu(baseline_sci, optimized_sci, alternative='two-sided')\n",
    "    \n",
    "    pooled_std = np.sqrt(((len(baseline_sci)-1)*baseline_sci.var() + \n",
    "                         (len(optimized_sci)-1)*optimized_sci.var()) / \n",
    "                        (len(baseline_sci)+len(optimized_sci)-2))\n",
    "    cohens_d = (baseline_mean - optimized_mean) / pooled_std\n",
    "    \n",
    "    diff_mean = baseline_mean - optimized_mean\n",
    "    diff_std = np.sqrt(baseline_std**2/len(baseline_sci) + optimized_std**2/len(optimized_sci))\n",
    "    ci_lower = diff_mean - 1.96 * diff_std\n",
    "    ci_upper = diff_mean + 1.96 * diff_std\n",
    "    \n",
    "    print(\"RQ3 Analysis: Carbon-Optimized Configuration Effectiveness\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Baseline (P1_E1): {baseline_mean:.3f} ± {baseline_std:.3f} gCO2eq/scenario (n={len(baseline_sci)})\")\n",
    "    print(f\"Carbon-Optimized (P3_E1): {optimized_mean:.3f} ± {optimized_std:.3f} gCO2eq/scenario (n={len(optimized_sci)})\")\n",
    "    print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "    print(f\"Absolute reduction: {diff_mean:.3f} gCO2eq/scenario\")\n",
    "    print(f\"95% CI for difference: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "    print(f\"\\nStatistical significance: p = {p_value:.4f}\")\n",
    "    print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\nConclusion: H3.0 is REJECTED. Carbon-optimized configuration significantly reduces SCI.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: H3.0 is NOT rejected. No significant reduction observed.\")\n",
    "    \n",
    "    rq3_results = pd.DataFrame({\n",
    "        'Metric': ['Baseline_Mean', 'Baseline_Std', 'Optimized_Mean', 'Optimized_Std', \n",
    "                  'Improvement_Percent', 'P_Value', 'Cohens_D', 'CI_Lower', 'CI_Upper'],\n",
    "        'Value': [baseline_mean, baseline_std, optimized_mean, optimized_std, \n",
    "                 improvement, p_value, cohens_d, ci_lower, ci_upper]\n",
    "    })\n",
    "    rq3_results.to_csv(results_dir / 'rq3_statistical_analysis.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"P3_E1 data not available for RQ3 analysis\")\n",
    "    improvement = None\n",
    "    p_value = None\n",
    "    cohens_d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary Tables\n",
    "\n",
    "Generating statistical summaries for inclusion in the thesis report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = []\n",
    "\n",
    "baseline_stats = {\n",
    "    'Configuration': 'P1_E1 Baseline',\n",
    "    'N': len(baseline_data),\n",
    "    'Mean_SCI': baseline_data['sci_score'].mean(),\n",
    "    'Std_SCI': baseline_data['sci_score'].std(),\n",
    "    'Median_SCI': baseline_data['sci_score'].median(),\n",
    "    'Min_SCI': baseline_data['sci_score'].min(),\n",
    "    'Max_SCI': baseline_data['sci_score'].max(),\n",
    "    'Mean_Power': baseline_data['powerjoular_power'].mean(),\n",
    "    'Mean_Duration': baseline_data['test_duration'].mean(),\n",
    "    'Success_Rate': baseline_data['test_success_rate'].mean()\n",
    "}\n",
    "summary_stats.append(baseline_stats)\n",
    "\n",
    "for exp_id in ['P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5']:\n",
    "    if exp_id in experiment_data:\n",
    "        exp_data = experiment_data[exp_id]\n",
    "        factor_col = experiments[exp_id].get('factor')\n",
    "        \n",
    "        if factor_col and factor_col in exp_data.columns:\n",
    "            for config in exp_data[factor_col].unique():\n",
    "                config_data_subset = exp_data[exp_data[factor_col] == config]\n",
    "                \n",
    "                config_stats = {\n",
    "                    'Configuration': f\"{exp_id}_{config}\",\n",
    "                    'N': len(config_data_subset),\n",
    "                    'Mean_SCI': config_data_subset['sci_score'].mean(),\n",
    "                    'Std_SCI': config_data_subset['sci_score'].std(),\n",
    "                    'Median_SCI': config_data_subset['sci_score'].median(),\n",
    "                    'Min_SCI': config_data_subset['sci_score'].min(),\n",
    "                    'Max_SCI': config_data_subset['sci_score'].max(),\n",
    "                    'Mean_Power': config_data_subset['powerjoular_power'].mean(),\n",
    "                    'Mean_Duration': config_data_subset['test_duration'].mean(),\n",
    "                    'Success_Rate': config_data_subset['test_success_rate'].mean()\n",
    "                }\n",
    "                summary_stats.append(config_stats)\n",
    "        else:\n",
    "            config_stats = {\n",
    "                'Configuration': f\"{exp_id}_no_compression\",\n",
    "                'N': len(exp_data),\n",
    "                'Mean_SCI': exp_data['sci_score'].mean(),\n",
    "                'Std_SCI': exp_data['sci_score'].std(),\n",
    "                'Median_SCI': exp_data['sci_score'].median(),\n",
    "                'Min_SCI': exp_data['sci_score'].min(),\n",
    "                'Max_SCI': exp_data['sci_score'].max(),\n",
    "                'Mean_Power': exp_data['powerjoular_power'].mean(),\n",
    "                'Mean_Duration': exp_data['test_duration'].mean(),\n",
    "                'Success_Rate': exp_data['test_success_rate'].mean()\n",
    "            }\n",
    "            summary_stats.append(config_stats)\n",
    "\n",
    "if 'P3_E1' in experiment_data:\n",
    "    p3_data = experiment_data['P3_E1']\n",
    "    p3_stats = {\n",
    "        'Configuration': 'P3_E1 Carbon-Optimized',\n",
    "        'N': len(p3_data),\n",
    "        'Mean_SCI': p3_data['sci_score'].mean(),\n",
    "        'Std_SCI': p3_data['sci_score'].std(),\n",
    "        'Median_SCI': p3_data['sci_score'].median(),\n",
    "        'Min_SCI': p3_data['sci_score'].min(),\n",
    "        'Max_SCI': p3_data['sci_score'].max(),\n",
    "        'Mean_Power': p3_data['powerjoular_power'].mean(),\n",
    "        'Mean_Duration': p3_data['test_duration'].mean(),\n",
    "        'Success_Rate': p3_data['test_success_rate'].mean()\n",
    "    }\n",
    "    summary_stats.append(p3_stats)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "if len(summary_df) > 0:\n",
    "    summary_df['Improvement_vs_Baseline'] = ((summary_df.iloc[0]['Mean_SCI'] - summary_df['Mean_SCI']) / \n",
    "                                             summary_df.iloc[0]['Mean_SCI']) * 100\n",
    "\n",
    "    summary_df.to_csv(results_dir / 'comprehensive_statistical_summary.csv', index=False)\n",
    "    \n",
    "    print(\"Top 5 Performing Configurations (by SCI reduction):\")\n",
    "    print(\"=\" * 55)\n",
    "    top_configs = summary_df.nlargest(5, 'Improvement_vs_Baseline')\n",
    "    for _, row in top_configs.iterrows():\n",
    "        print(f\"{row['Configuration']}: {row['Mean_SCI']:.3f} gCO2eq/scenario ({row['Improvement_vs_Baseline']:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nComprehensive summary saved to {results_dir / 'comprehensive_statistical_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing Summary\n",
    "\n",
    "Providing definitive conclusions for both research questions based on statistical evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HYPOTHESIS TESTING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not rq2_df.empty:\n",
    "    significant_configs = rq2_df[rq2_df['significant'] & (rq2_df['improvement_percent'] > 0)]\n",
    "    print(f\"\\nRQ2 - Impact of Application-Level Configurations:\")\n",
    "    print(f\"Configurations tested: {len(rq2_df)}\")\n",
    "    print(f\"Significant improvements: {len(significant_configs)}\")\n",
    "    print(f\"Best improvement: {rq2_df['improvement_percent'].max():.1f}%\")\n",
    "    \n",
    "    if len(significant_configs) > 0:\n",
    "        print(\"\\nH1.0 REJECTED: At least one configuration significantly affects SCI score\")\n",
    "        print(\"H1.a ACCEPTED: Configuration changes can significantly reduce carbon emissions\")\n",
    "        \n",
    "        print(\"\\nSignificant configurations:\")\n",
    "        for _, row in significant_configs.iterrows():\n",
    "            print(f\"  {row['experiment']}_{row['configuration']}: {row['improvement_percent']:+.1f}% (p={row['p_value']:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nH1.0 NOT REJECTED: No significant configuration effects detected\")\n",
    "        print(\"H1.a REJECTED: Configuration changes do not significantly affect SCI score\")\n",
    "else:\n",
    "    print(\"\\nRQ2 - No comparison data available\")\n",
    "    significant_configs = pd.DataFrame()\n",
    "\n",
    "if 'P3_E1' in experiment_data and improvement is not None:\n",
    "    print(f\"\\n\\nRQ3 - Carbon-Optimized Configuration Effectiveness:\")\n",
    "    print(f\"Improvement achieved: {improvement:.1f}%\")\n",
    "    print(f\"Statistical significance: p = {p_value:.4f}\")\n",
    "    print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05 and improvement > 0:\n",
    "        print(\"\\nH2.0 REJECTED: The combined optimal SCI configuration significantly affects SCI score\")\n",
    "        print(\"H2.a ACCEPTED: The carbon-optimized configuration has a significant effect\")\n",
    "        \n",
    "        if cohens_d >= 0.8:\n",
    "            effect_interpretation = \"large\"\n",
    "        elif cohens_d >= 0.5:\n",
    "            effect_interpretation = \"medium\"\n",
    "        elif cohens_d >= 0.2:\n",
    "            effect_interpretation = \"small\"\n",
    "        else:\n",
    "            effect_interpretation = \"negligible\"\n",
    "        \n",
    "        print(f\"Effect size interpretation: {effect_interpretation} practical significance\")\n",
    "    else:\n",
    "        print(\"\\nH2.0 NOT REJECTED: No significant effect from the combined optimal SCI configuration\")\n",
    "        print(\"H2.a REJECTED: The carbon-optimized configuration does not have a significant effect\")\n",
    "else:\n",
    "    print(\"\\n\\nRQ3 - Carbon-Optimized configuration data not available\")\n",
    "\n",
    "hypothesis_results = {\n",
    "    'RQ2_H10_Status': 'Rejected' if not rq2_df.empty and len(significant_configs) > 0 else 'Not Rejected',\n",
    "    'RQ2_H1a_Status': 'Accepted' if not rq2_df.empty and len(significant_configs) > 0 else 'Rejected',\n",
    "    'RQ2_Significant_Configs': len(significant_configs) if not rq2_df.empty else 0,\n",
    "    'RQ2_Best_Improvement': rq2_df['improvement_percent'].max() if not rq2_df.empty else None,\n",
    "    'RQ3_H20_Status': 'Rejected' if (improvement is not None and p_value < 0.05 and improvement > 0) else 'Not Rejected',\n",
    "    'RQ3_H2a_Status': 'Accepted' if (improvement is not None and p_value < 0.05 and improvement > 0) else 'Rejected',\n",
    "    'RQ3_Improvement_Percent': improvement,\n",
    "    'RQ3_P_Value': p_value,\n",
    "    'RQ3_Effect_Size': cohens_d\n",
    "}\n",
    "\n",
    "hypothesis_df = pd.DataFrame([hypothesis_results])\n",
    "hypothesis_df.to_csv(results_dir / 'hypothesis_testing_results.csv', index=False)\n",
    "\n",
    "print(f\"\\nHypothesis testing results saved to {results_dir / 'hypothesis_testing_results.csv'}\")\n",
    "print(f\"\\nAnalysis complete. All figures saved to {figures_dir}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for csv_file in results_dir.glob('*.csv'):\n",
    "    print(f\"- {csv_file.name}\")\n",
    "print(\"\\nGenerated figures:\")\n",
    "for png_file in figures_dir.glob('*.png'):\n",
    "    print(f\"- {png_file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
