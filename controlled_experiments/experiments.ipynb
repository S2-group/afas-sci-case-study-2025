{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFAS SB Carbon Reduction Experiment Analysis\n",
    "\n",
    "**Empirical Assessment of Carbon Reduction Strategies in Enterprise Software Systems**\n",
    "\n",
    "This notebook presents the comprehensive statistical analysis of controlled experiments investigating the impact of application-level configurations on carbon emissions in AFAS SB. The analysis addresses Research Questions 2 and 3 from the thesis, employing rigorous statistical methods to evaluate configuration impacts and optimization potential.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "**RQ2**: How do application-level configuration settings affect the rate of carbon emissions of AFAS SB?\n",
    "\n",
    "**RQ3**: To what extent can the rate of carbon emissions of AFAS SB be reduced by combining optimal application-level configurations while maintaining system performance and functionality?\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "**H2.0**: Application-level configuration changes do not significantly affect the SCI score  \n",
    "**H2.1**: At least one configuration change significantly affects the SCI score  \n",
    "\n",
    "**H3.0**: Combined optimal configurations cannot significantly reduce the SCI score  \n",
    "**H3.1**: Combined optimal configurations can significantly reduce the SCI score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "Loading experimental data from the controlled environment and calculating SCI scores according to the Green Software Foundation specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, levene, mannwhitneyu, kruskal, ttest_ind, f_oneway\n",
    "from scipy.stats import pearsonr, spearmanr, bootstrap\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import ttest_power\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "from matplotlib.patches import Rectangle\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.figsize': (10, 6),\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "figures_dir = Path('figures')\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "results_dir = Path('results_data')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Analysis environment initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCI Calculator and Data Loading Functions\n",
    "\n",
    "Implementing the Software Carbon Intensity calculator according to Green Software Foundation specification and data loading utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCICalculator:\n",
    "    \"\"\"Calculate SCI scores according to Green Software Foundation specification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.GRID_CARBON_INTENSITY = 272  # gCO2e/kWh, April 2025, The Netherlands\n",
    "        self.TOTAL_EMBODIED_EMISSIONS = 1449.84  # kg CO2e\n",
    "        self.HARDWARE_LIFESPAN_HOURS = 4 * 365 * 24  # 4 years\n",
    "        self.VM_CPU_CORES = 8\n",
    "        self.TOTAL_SERVER_CORES = 24\n",
    "        \n",
    "    def calculate_sci(self, power_watts: float, duration_seconds: float, \n",
    "                     functional_units: int) -> Dict[str, float]:\n",
    "        \"\"\"Calculate complete SCI breakdown according to specification.\"\"\"\n",
    "        \n",
    "        # Operational Emissions (O = E × I)\n",
    "        energy_kwh = (power_watts * duration_seconds) / (1000 * 3600)\n",
    "        operational_emissions = energy_kwh * self.GRID_CARBON_INTENSITY\n",
    "        \n",
    "        # Embodied Emissions (M = TE × (TiR/EL) × (RR/ToR))\n",
    "        time_reserved_hours = duration_seconds / 3600\n",
    "        resource_ratio = self.VM_CPU_CORES / self.TOTAL_SERVER_CORES\n",
    "        \n",
    "        embodied_emissions = (\n",
    "            self.TOTAL_EMBODIED_EMISSIONS * 1000 *\n",
    "            (time_reserved_hours / self.HARDWARE_LIFESPAN_HOURS) *\n",
    "            resource_ratio\n",
    "        )\n",
    "        \n",
    "        # SCI Score = (O + M) / R\n",
    "        total_emissions = operational_emissions + embodied_emissions\n",
    "        sci_score = total_emissions / functional_units if functional_units > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'energy_kwh': energy_kwh,\n",
    "            'operational_emissions': operational_emissions,\n",
    "            'embodied_emissions': embodied_emissions,\n",
    "            'total_emissions': total_emissions,\n",
    "            'sci_score': sci_score\n",
    "        }\n",
    "\n",
    "def load_experiment_data(base_path: str = \"experiment_runner/afas-sb/experiments\") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load all experimental data and calculate SCI scores\"\"\"\n",
    "    \n",
    "    sci_calc = SCICalculator()\n",
    "    data = {}\n",
    "    \n",
    "    experiments = {\n",
    "        'P1_E1': {'name': 'Baseline Configuration', 'category': 'baseline'},\n",
    "        'P2_E1': {'name': 'Parallelism Configuration', 'category': 'parallelism', 'factor': 'parallelism_config'},\n",
    "        'P2_E2': {'name': 'Logging Configuration', 'category': 'logging', 'factor': 'logging_config'},\n",
    "        'P2_E3': {'name': 'Cache Configuration', 'category': 'caching', 'factor': 'cache_config'},\n",
    "        'P2_E4': {'name': 'Compression Configuration', 'category': 'compression', 'factor': 'compression_config'},\n",
    "        'P2_E5': {'name': 'Garbage Collection Configuration', 'category': 'gc', 'factor': 'gc_config'},\n",
    "        'P3_E1': {'name': 'Carbon-Aware Configuration', 'category': 'optimized', 'factor': 'optimization_config'}\n",
    "    }\n",
    "    \n",
    "    for exp_id in experiments.keys():\n",
    "        run_table_path = Path(base_path) / exp_id / 'run_table.csv'\n",
    "        \n",
    "        if run_table_path.exists():\n",
    "            df = pd.read_csv(run_table_path)\n",
    "            df = df[df['__done'] == 'DONE'].copy()\n",
    "            \n",
    "            if len(df) > 0:\n",
    "                sci_results = df.apply(lambda row: sci_calc.calculate_sci(\n",
    "                    power_watts=row.get('powerjoular_power', 0),\n",
    "                    duration_seconds=row.get('test_duration', 0),\n",
    "                    functional_units=row.get('scenario_count', 1)\n",
    "                ), axis=1)\n",
    "                \n",
    "                for key in ['energy_kwh', 'operational_emissions', 'embodied_emissions', 'total_emissions', 'sci_score']:\n",
    "                    df[key] = [result[key] for result in sci_results]\n",
    "                \n",
    "                df['experiment_id'] = exp_id\n",
    "                df['experiment_name'] = experiments[exp_id]['name']\n",
    "                df['category'] = experiments[exp_id]['category']\n",
    "                \n",
    "                if exp_id == 'P3_E1' and 'optimization_config' in df.columns:\n",
    "                    df = df[df['optimization_config'] == 'carbon_optimized'].copy()\n",
    "                \n",
    "                data[exp_id] = df\n",
    "                print(f\"Loaded {exp_id}: {len(df)} runs, mean SCI: {df['sci_score'].mean():.3f} gCO2e/scenario\")\n",
    "    \n",
    "    return data, experiments\n",
    "\n",
    "experiment_data, experiments = load_experiment_data()\n",
    "all_data = pd.concat(experiment_data.values(), ignore_index=True)\n",
    "print(f\"\\nTotal runs loaded: {len(all_data)}\")\n",
    "print(f\"Overall mean SCI: {all_data['sci_score'].mean():.3f} gCO2e/scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Baseline Reference\n",
    "\n",
    "Establishing the baseline reference (P1_E1) for comparative analysis and preparing configuration-specific datasets for RQ2 analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = experiment_data['P1_E1']\n",
    "baseline_sci_mean = baseline_data['sci_score'].mean()\n",
    "baseline_sci_std = baseline_data['sci_score'].std()\n",
    "\n",
    "print(f\"Baseline SCI: {baseline_sci_mean:.3f} ± {baseline_sci_std:.3f} gCO2e/scenario (n={len(baseline_data)})\")\n",
    "\n",
    "config_experiments = {\n",
    "    'P2_E1': 'parallelism_config',\n",
    "    'P2_E2': 'logging_config',\n",
    "    'P2_E3': 'cache_config', \n",
    "    'P2_E5': 'gc_config'\n",
    "}\n",
    "\n",
    "config_data = {}\n",
    "for exp_id, config_column in config_experiments.items():\n",
    "    if exp_id in experiment_data and config_column in experiment_data[exp_id].columns:\n",
    "        exp_df = experiment_data[exp_id]\n",
    "        configs = exp_df[config_column].unique()\n",
    "        \n",
    "        for config in configs:\n",
    "            config_df = exp_df[exp_df[config_column] == config].copy()\n",
    "            config_key = f\"{exp_id}_{config}\"\n",
    "            config_data[config_key] = config_df\n",
    "            print(f\"{config_key}: {len(config_df)} runs\")\n",
    "\n",
    "if 'P2_E4' in experiment_data:\n",
    "    config_data['P2_E4_no_compression'] = experiment_data['P2_E4']\n",
    "    print(f\"P2_E4_no_compression: {len(experiment_data['P2_E4'])} runs\")\n",
    "    \n",
    "if 'P3_E1' in experiment_data:\n",
    "    config_data['P3_E1_carbon_optimized'] = experiment_data['P3_E1']\n",
    "    print(f\"P3_E1_carbon_optimized: {len(experiment_data['P3_E1'])} runs\")\n",
    "\n",
    "print(f\"\\nTotal configurations for analysis: {len(config_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Question 2 Analysis\n",
    "\n",
    "### Statistical Comparison Framework\n",
    "\n",
    "Implementing comprehensive statistical comparison functions to analyze configuration effects against baseline with appropriate test selection based on data distribution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_comparison(config_data: pd.DataFrame, baseline_data: pd.DataFrame, \n",
    "                         metric: str = 'sci_score') -> Dict:\n",
    "    \"\"\"Perform comprehensive statistical comparison between configuration and baseline\"\"\"\n",
    "    \n",
    "    config_values = config_data[metric].dropna()\n",
    "    baseline_values = baseline_data[metric].dropna()\n",
    "    \n",
    "    if len(config_values) < 3 or len(baseline_values) < 3:\n",
    "        return {'error': 'Insufficient data'}\n",
    "    \n",
    "    config_normal = shapiro(config_values)[1] > 0.05\n",
    "    baseline_normal = shapiro(baseline_values)[1] > 0.05\n",
    "    \n",
    "    if config_normal and baseline_normal:\n",
    "        equal_var = levene(config_values, baseline_values)[1] > 0.05\n",
    "        stat, p_value = ttest_ind(config_values, baseline_values, equal_var=equal_var)\n",
    "        test_type = f\"t-test ({'equal' if equal_var else 'unequal'} var)\"\n",
    "        \n",
    "        pooled_std = np.sqrt(((len(config_values)-1)*config_values.var() + \n",
    "                             (len(baseline_values)-1)*baseline_values.var()) / \n",
    "                            (len(config_values)+len(baseline_values)-2))\n",
    "        effect_size = (baseline_values.mean() - config_values.mean()) / pooled_std\n",
    "    else:\n",
    "        stat, p_value = mannwhitneyu(config_values, baseline_values, alternative='two-sided')\n",
    "        test_type = \"Mann-Whitney U\"\n",
    "        n1, n2 = len(baseline_values), len(config_values)\n",
    "        cliff_delta = (sum(x > y for x in baseline_values for y in config_values) - \n",
    "                      sum(x < y for x in baseline_values for y in config_values)) / (n1 * n2)\n",
    "        effect_size = cliff_delta\n",
    "    \n",
    "    improvement = ((baseline_values.mean() - config_values.mean()) / \n",
    "                  baseline_values.mean()) * 100\n",
    "    \n",
    "    return {\n",
    "        'config_mean': config_values.mean(),\n",
    "        'config_std': config_values.std(),\n",
    "        'baseline_mean': baseline_values.mean(),\n",
    "        'baseline_std': baseline_values.std(),\n",
    "        'improvement_pct': improvement,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': effect_size,\n",
    "        'test_type': test_type,\n",
    "        'significant': p_value < 0.05,\n",
    "        'config_n': len(config_values),\n",
    "        'baseline_n': len(baseline_values)\n",
    "    }\n",
    "\n",
    "rq2_results = {}\n",
    "metrics = ['sci_score', 'powerjoular_power', 'test_duration', 'powerjoular_util']\n",
    "\n",
    "for config_name, config_df in config_data.items():\n",
    "    rq2_results[config_name] = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in config_df.columns and metric in baseline_data.columns:\n",
    "            result = statistical_comparison(config_df, baseline_data, metric)\n",
    "            rq2_results[config_name][metric] = result\n",
    "\n",
    "print(\"Statistical analysis completed for all configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Comparison Correction and Advanced Statistical Testing\n",
    "\n",
    "Implementing Bonferroni and Holm corrections for multiple comparisons to maintain statistical rigor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_for_correction = []\n",
    "test_descriptions = []\n",
    "\n",
    "for config_name, metrics_data in rq2_results.items():\n",
    "    for metric, result in metrics_data.items():\n",
    "        if not result.get('error') and 'p_value' in result:\n",
    "            p_values_for_correction.append(result['p_value'])\n",
    "            test_descriptions.append(f\"{config_name}_{metric}\")\n",
    "\n",
    "if p_values_for_correction:\n",
    "    bonferroni_rejected, bonferroni_corrected, _, bonferroni_alpha = multipletests(\n",
    "        p_values_for_correction, alpha=0.05, method='bonferroni'\n",
    "    )\n",
    "    \n",
    "    holm_rejected, holm_corrected, _, holm_alpha = multipletests(\n",
    "        p_values_for_correction, alpha=0.05, method='holm'\n",
    "    )\n",
    "    \n",
    "    correction_results = pd.DataFrame({\n",
    "        'Test': test_descriptions,\n",
    "        'Original_P_Value': p_values_for_correction,\n",
    "        'Bonferroni_Corrected_P': bonferroni_corrected,\n",
    "        'Bonferroni_Significant': bonferroni_rejected,\n",
    "        'Holm_Corrected_P': holm_corrected,\n",
    "        'Holm_Significant': holm_rejected\n",
    "    })\n",
    "    \n",
    "    correction_results.to_csv(results_dir / 'multiple_comparison_corrections.csv', index=False)\n",
    "    \n",
    "    print(f\"Multiple Comparison Correction Applied:\")\n",
    "    print(f\"Total tests: {len(p_values_for_correction)}\")\n",
    "    print(f\"Original alpha: 0.05\")\n",
    "    print(f\"Bonferroni alpha: {bonferroni_alpha:.6f}\")\n",
    "    print(f\"Significant after Bonferroni: {sum(bonferroni_rejected)}\")\n",
    "    print(f\"Significant after Holm: {sum(holm_rejected)}\")\n",
    "    \n",
    "    significant_after_correction = correction_results[\n",
    "        correction_results['Holm_Significant'] == True\n",
    "    ].sort_values('Holm_Corrected_P')\n",
    "    \n",
    "    if len(significant_after_correction) > 0:\n",
    "        print(f\"\\nSignificant results after Holm correction:\")\n",
    "        for _, row in significant_after_correction.head(10).iterrows():\n",
    "            print(f\"  {row['Test']}: p = {row['Holm_Corrected_P']:.6f}\")\n",
    "    else:\n",
    "        print(\"\\nNo results remain significant after multiple comparison correction.\")\n",
    "else:\n",
    "    print(\"No p-values available for multiple comparison correction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals and Robustness Analysis\n",
    "\n",
    "Using bootstrap resampling to estimate confidence intervals for effect sizes and validate the robustness of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_effect_size(group1, group2, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for effect sizes\"\"\"\n",
    "    \n",
    "    def cohen_d(x, y):\n",
    "        \"\"\"Calculate Cohen's d effect size\"\"\"\n",
    "        pooled_std = np.sqrt(((len(x) - 1) * np.var(x, ddof=1) + \n",
    "                             (len(y) - 1) * np.var(y, ddof=1)) / \n",
    "                            (len(x) + len(y) - 2))\n",
    "        return (np.mean(x) - np.mean(y)) / pooled_std\n",
    "    \n",
    "    bootstrap_effects = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_group1 = np.random.choice(group1, size=len(group1), replace=True)\n",
    "        boot_group2 = np.random.choice(group2, size=len(group2), replace=True)\n",
    "        \n",
    "        effect = cohen_d(boot_group1, boot_group2)\n",
    "        bootstrap_effects.append(effect)\n",
    "    \n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_effects, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_effects, upper_percentile)\n",
    "    \n",
    "    return {\n",
    "        'effect_size': cohen_d(group1, group2),\n",
    "        'bootstrap_mean': np.mean(bootstrap_effects),\n",
    "        'bootstrap_std': np.std(bootstrap_effects),\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'bootstrap_effects': bootstrap_effects\n",
    "    }\n",
    "\n",
    "bootstrap_results = []\n",
    "\n",
    "for config_name, config_df in config_data.items():\n",
    "    if 'sci_score' in config_df.columns:\n",
    "        config_sci = config_df['sci_score'].dropna()\n",
    "        baseline_sci = baseline_data['sci_score'].dropna()\n",
    "        \n",
    "        if len(config_sci) >= 3 and len(baseline_sci) >= 3:\n",
    "            boot_result = bootstrap_effect_size(baseline_sci, config_sci)\n",
    "            \n",
    "            ci_significant = not (boot_result['ci_lower'] <= 0 <= boot_result['ci_upper'])\n",
    "            \n",
    "            bootstrap_results.append({\n",
    "                'Configuration': config_name,\n",
    "                'Effect_Size': boot_result['effect_size'],\n",
    "                'Bootstrap_Mean': boot_result['bootstrap_mean'],\n",
    "                'Bootstrap_Std': boot_result['bootstrap_std'],\n",
    "                'CI_Lower': boot_result['ci_lower'],\n",
    "                'CI_Upper': boot_result['ci_upper'],\n",
    "                'CI_Significant': ci_significant\n",
    "            })\n",
    "\n",
    "bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "if not bootstrap_df.empty:\n",
    "    bootstrap_df = bootstrap_df.sort_values('Effect_Size', ascending=False)\n",
    "    bootstrap_df.to_csv(results_dir / 'bootstrap_effect_sizes.csv', index=False)\n",
    "    \n",
    "    print(\"Bootstrap Effect Size Analysis (SCI Score):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for _, row in bootstrap_df.head(10).iterrows():\n",
    "        significance = \"***\" if row['CI_Significant'] else \"n.s.\"\n",
    "        print(f\"{row['Configuration'][:25]:25} Effect: {row['Effect_Size']:6.3f} \"\n",
    "              f\"95% CI: [{row['CI_Lower']:6.3f}, {row['CI_Upper']:6.3f}] {significance}\")\n",
    "    \n",
    "    robust_significant = bootstrap_df[bootstrap_df['CI_Significant'] == True]\n",
    "    print(f\"\\nConfigurations with robust significant effects: {len(robust_significant)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No bootstrap results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Analysis and Sample Size Assessment\n",
    "\n",
    "Conducting post-hoc power analysis to assess the adequacy of sample sizes and identify potential Type II errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_analysis_results = []\n",
    "\n",
    "for exp_id, exp_df in experiment_data.items():\n",
    "    if exp_id != 'P1_E1':\n",
    "        exp_sci = exp_df['sci_score'].dropna()\n",
    "        baseline_sci = baseline_data['sci_score'].dropna()\n",
    "        \n",
    "        if len(exp_sci) >= 3 and len(baseline_sci) >= 3:\n",
    "            pooled_std = np.sqrt(((len(exp_sci) - 1) * exp_sci.var() + \n",
    "                                 (len(baseline_sci) - 1) * baseline_sci.var()) / \n",
    "                                (len(exp_sci) + len(baseline_sci) - 2))\n",
    "            \n",
    "            observed_effect = abs((baseline_sci.mean() - exp_sci.mean()) / pooled_std)\n",
    "            \n",
    "            achieved_power = ttest_power(\n",
    "                effect_size=observed_effect,\n",
    "                nobs=min(len(exp_sci), len(baseline_sci)),\n",
    "                alpha=0.05,\n",
    "                alternative='two-sided'\n",
    "            )\n",
    "            \n",
    "            required_n_80 = stats.norm.ppf(0.975) + stats.norm.ppf(0.8)\n",
    "            required_n_80 = int(np.ceil((required_n_80 / observed_effect) ** 2)) if observed_effect > 0 else np.inf\n",
    "            \n",
    "            required_n_90 = stats.norm.ppf(0.975) + stats.norm.ppf(0.9)\n",
    "            required_n_90 = int(np.ceil((required_n_90 / observed_effect) ** 2)) if observed_effect > 0 else np.inf\n",
    "            \n",
    "            power_analysis_results.append({\n",
    "                'Experiment': exp_id,\n",
    "                'Sample_Size': len(exp_sci),\n",
    "                'Observed_Effect_Size': observed_effect,\n",
    "                'Achieved_Power': achieved_power,\n",
    "                'Required_N_80_Power': required_n_80,\n",
    "                'Required_N_90_Power': required_n_90,\n",
    "                'Adequate_Power_80': achieved_power >= 0.8,\n",
    "                'Adequate_Power_90': achieved_power >= 0.9\n",
    "            })\n",
    "\n",
    "power_df = pd.DataFrame(power_analysis_results)\n",
    "if not power_df.empty:\n",
    "    power_df.to_csv(results_dir / 'power_analysis_results.csv', index=False)\n",
    "    \n",
    "    print(\"Post-hoc Power Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Experiment':<12} {'N':<4} {'Effect':<8} {'Power':<8} {'80% Power':<11} {'90% Power':<11}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for _, row in power_df.iterrows():\n",
    "        power_80_status = \"✓\" if row['Adequate_Power_80'] else \"✗\"\n",
    "        power_90_status = \"✓\" if row['Adequate_Power_90'] else \"✗\"\n",
    "        \n",
    "        print(f\"{row['Experiment']:<12} {row['Sample_Size']:<4} \"\n",
    "              f\"{row['Observed_Effect_Size']:<8.3f} {row['Achieved_Power']:<8.3f} \"\n",
    "              f\"{power_80_status:<11} {power_90_status:<11}\")\n",
    "    \n",
    "    adequate_80 = power_df['Adequate_Power_80'].sum()\n",
    "    adequate_90 = power_df['Adequate_Power_90'].sum()\n",
    "    total_exp = len(power_df)\n",
    "    \n",
    "    print(f\"\\nPower Analysis Summary:\")\n",
    "    print(f\"Experiments with adequate power (≥80%): {adequate_80}/{total_exp} ({adequate_80/total_exp*100:.1f}%)\")\n",
    "    print(f\"Experiments with high power (≥90%): {adequate_90}/{total_exp} ({adequate_90/total_exp*100:.1f}%)\")\n",
    "    \n",
    "    low_power = power_df[power_df['Achieved_Power'] < 0.8]\n",
    "    if len(low_power) > 0:\n",
    "        print(f\"\\nExperiments with potentially inadequate power (risk of Type II error):\")\n",
    "        for _, row in low_power.iterrows():\n",
    "            print(f\"   {row['Experiment']}: Power = {row['Achieved_Power']:.3f}, \"\n",
    "                  f\"Recommend n ≥ {row['Required_N_80_Power']} for 80% power\")\n",
    "else:\n",
    "    print(\"No power analysis results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Application-Level Configuration Settings\n",
    "\n",
    "Analyzing how different configuration parameters affect carbon emissions compared to the baseline (P1_E1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline SCI: {baseline_sci_mean:.3f} ± {baseline_sci_std:.3f} gCO2e/scenario (n={len(baseline_data)})\")\n",
    "print(\"\\nRQ2 Analysis: Comparing configurations against baseline\\n\")\n",
    "\n",
    "rq2_comparison_results = []\n",
    "\n",
    "for exp_id in ['P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5']:\n",
    "    if exp_id in experiment_data:\n",
    "        exp_data = experiment_data[exp_id]\n",
    "        factor_col = experiments[exp_id].get('factor')\n",
    "        \n",
    "        print(f\"\\n{experiments[exp_id]['name']} ({exp_id}):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if factor_col and factor_col in exp_data.columns:\n",
    "            configurations = exp_data[factor_col].unique()\n",
    "            \n",
    "            for config in configurations:\n",
    "                config_data_subset = exp_data[exp_data[factor_col] == config]\n",
    "                config_sci_mean = config_data_subset['sci_score'].mean()\n",
    "                config_sci_std = config_data_subset['sci_score'].std()\n",
    "                \n",
    "                improvement = ((baseline_sci_mean - config_sci_mean) / baseline_sci_mean) * 100\n",
    "                \n",
    "                stat, p_value = mannwhitneyu(baseline_data['sci_score'], config_data_subset['sci_score'], alternative='two-sided')\n",
    "                \n",
    "                n1, n2 = len(baseline_data), len(config_data_subset)\n",
    "                cliff_delta = (sum(x > y for x in baseline_data['sci_score'] for y in config_data_subset['sci_score']) - \n",
    "                             sum(x < y for x in baseline_data['sci_score'] for y in config_data_subset['sci_score'])) / (n1 * n2)\n",
    "                \n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"n.s.\"\n",
    "                \n",
    "                print(f\"  {config}: {config_sci_mean:.3f} ± {config_sci_std:.3f} (n={len(config_data_subset)})\")\n",
    "                print(f\"    Improvement: {improvement:+.1f}% | p={p_value:.4f} {significance} | δ={cliff_delta:.3f}\")\n",
    "                \n",
    "                rq2_comparison_results.append({\n",
    "                    'experiment': exp_id,\n",
    "                    'experiment_name': experiments[exp_id]['name'],\n",
    "                    'configuration': config,\n",
    "                    'baseline_mean': baseline_sci_mean,\n",
    "                    'config_mean': config_sci_mean,\n",
    "                    'improvement_percent': improvement,\n",
    "                    'p_value': p_value,\n",
    "                    'cliff_delta': cliff_delta,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'n_baseline': len(baseline_data),\n",
    "                    'n_config': len(config_data_subset)\n",
    "                })\n",
    "        else:\n",
    "            exp_sci_mean = exp_data['sci_score'].mean()\n",
    "            exp_sci_std = exp_data['sci_score'].std()\n",
    "            improvement = ((baseline_sci_mean - exp_sci_mean) / baseline_sci_mean) * 100\n",
    "            \n",
    "            stat, p_value = mannwhitneyu(baseline_data['sci_score'], exp_data['sci_score'], alternative='two-sided')\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"n.s.\"\n",
    "            print(f\"  Overall: {exp_sci_mean:.3f} ± {exp_sci_std:.3f} (n={len(exp_data)})\")\n",
    "            print(f\"    Improvement: {improvement:+.1f}% | p={p_value:.4f} {significance}\")\n",
    "\n",
    "rq2_df = pd.DataFrame(rq2_comparison_results)\n",
    "if not rq2_df.empty:\n",
    "    rq2_df.to_csv(results_dir / 'rq2_statistical_analysis.csv', index=False)\n",
    "\n",
    "    significant_improvements = rq2_df[rq2_df['significant'] & (rq2_df['improvement_percent'] > 0)]\n",
    "    print(f\"\\nRQ2 Summary: {len(significant_improvements)} out of {len(rq2_df)} configurations show significant improvement\")\n",
    "    if len(rq2_df) > 0:\n",
    "        print(f\"Best improvement: {rq2_df['improvement_percent'].max():.1f}% (p={rq2_df.loc[rq2_df['improvement_percent'].idxmax(), 'p_value']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Statistical Analysis Results\n",
    "\n",
    "Analyzing significance and effect sizes across all configuration metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_results = []\n",
    "\n",
    "for config_name, metrics_data in rq2_results.items():\n",
    "    sci_result = metrics_data.get('sci_score', {})\n",
    "    if sci_result.get('significant', False) and not sci_result.get('error'):\n",
    "        significant_results.append({\n",
    "            'Configuration': config_name,\n",
    "            'SCI_Mean': sci_result['config_mean'],\n",
    "            'SCI_Std': sci_result['config_std'],\n",
    "            'Improvement_%': sci_result['improvement_pct'],\n",
    "            'P_Value': sci_result['p_value'],\n",
    "            'Effect_Size': sci_result['effect_size'],\n",
    "            'Test_Type': sci_result['test_type'],\n",
    "            'Sample_Size': sci_result['config_n']\n",
    "        })\n",
    "\n",
    "significant_df = pd.DataFrame(significant_results)\n",
    "if not significant_df.empty:\n",
    "    significant_df = significant_df.sort_values('Improvement_%', ascending=False)\n",
    "    significant_df.to_csv(results_dir / 'rq2_significant_configurations.csv', index=False)\n",
    "    \n",
    "    print(\"Significant SCI Improvements vs Baseline:\")\n",
    "    print(significant_df[['Configuration', 'Improvement_%', 'P_Value']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No statistically significant SCI improvements found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCI Distribution Analysis by Configuration\n",
    "\n",
    "Visualizing the distribution of SCI scores across different configurations using violin plots to show both central tendency and distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "\n",
    "for _, row in baseline_data.iterrows():\n",
    "    plot_data.append({\n",
    "        'SCI_Score': row['sci_score'],\n",
    "        'Configuration': 'Baseline',\n",
    "        'Experiment': 'P1_E1'\n",
    "    })\n",
    "\n",
    "for exp_id in ['P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5']:\n",
    "    if exp_id in experiment_data:\n",
    "        exp_data = experiment_data[exp_id]\n",
    "        factor_col = experiments[exp_id].get('factor')\n",
    "\n",
    "        if factor_col and factor_col in exp_data.columns:\n",
    "            for _, row in exp_data.iterrows():\n",
    "                plot_data.append({\n",
    "                    'SCI_Score': row['sci_score'],\n",
    "                    'Configuration': row[factor_col],\n",
    "                    'Experiment': exp_id\n",
    "                })\n",
    "\n",
    "if 'P3_E1' in experiment_data:\n",
    "    for _, row in experiment_data['P3_E1'].iterrows():\n",
    "        plot_data.append({\n",
    "            'SCI_Score': row['sci_score'],\n",
    "            'Configuration': 'Carbon-Optimized',\n",
    "            'Experiment': 'P3_E1'\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "experiment_order = ['P1_E1', 'P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5', 'P3_E1']\n",
    "\n",
    "config_to_experiment = plot_df.set_index('Configuration')['Experiment'].to_dict()\n",
    "\n",
    "unique_configs = plot_df['Configuration'].unique()\n",
    "config_order = sorted(unique_configs, key=lambda c: experiment_order.index(config_to_experiment[c]))\n",
    "\n",
    "experiment_palette = sns.color_palette('tab10', len(experiment_order))\n",
    "experiment_color_map = dict(zip(experiment_order, experiment_palette))\n",
    "\n",
    "violin_plot = sns.violinplot(\n",
    "    data=plot_df,\n",
    "    x='Configuration',\n",
    "    y='SCI_Score',\n",
    "    order=config_order,\n",
    "    inner='box',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(config_order, rotation=45)\n",
    "ax.set_title('SCI Score Distribution by Configuration', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Configuration', fontsize=14)\n",
    "ax.set_ylabel('SCI Score (gCO₂e/scenario)', fontsize=14)\n",
    "\n",
    "ax.axhline(y=baseline_sci_mean, color='red', linestyle='--', alpha=0.7,\n",
    "           label=f'Baseline Mean: {baseline_sci_mean:.3f}')\n",
    "\n",
    "for i, config in enumerate(config_order):\n",
    "    experiment = config_to_experiment[config]\n",
    "    color = experiment_color_map[experiment]\n",
    "    violin_plot.collections[i * 2].set_facecolor(color)\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color, label=exp) for exp, color in experiment_color_map.items()]\n",
    "ax.legend(handles=legend_patches, title='Experiment', bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'sci_distribution_by_configuration.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Violin plot saved to {figures_dir / 'sci_distribution_by_configuration.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Effect Heatmap\n",
    "\n",
    "Creating a heatmap showing the effect sizes and significance levels of different configurations across multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = []\n",
    "metrics_display = {\n",
    "    'sci_score': 'SCI Score',\n",
    "    'powerjoular_power': 'Power (W)',\n",
    "    'test_duration': 'Duration (s)',\n",
    "    'powerjoular_util': 'CPU Usage (%)'\n",
    "}\n",
    "\n",
    "for config_name, metrics_data in rq2_results.items():\n",
    "    for metric, metric_display in metrics_display.items():\n",
    "        result = metrics_data.get(metric, {})\n",
    "        if not result.get('error'):\n",
    "            heatmap_data.append({\n",
    "                'Configuration': config_name.replace('_', ' '),\n",
    "                'Metric': metric_display,\n",
    "                'Improvement_%': result.get('improvement_pct', 0),\n",
    "                'Effect_Size': abs(result.get('effect_size', 0)),\n",
    "                'Significant': result.get('significant', False),\n",
    "                'P_Value': result.get('p_value', 1.0)\n",
    "            })\n",
    "\n",
    "heatmap_df = pd.DataFrame(heatmap_data)\n",
    "\n",
    "if not heatmap_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    pivot_improvement = heatmap_df.pivot(index='Configuration', columns='Metric', values='Improvement_%')\n",
    "    sns.heatmap(pivot_improvement, annot=True, fmt='.1f', cmap='RdYlGn', center=0,\n",
    "                ax=ax1, cbar_kws={'label': 'Improvement %'})\n",
    "    ax1.set_title('Configuration Impact: Improvement Percentage vs Baseline', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Metrics', fontweight='bold')\n",
    "    ax1.set_ylabel('Configuration', fontweight='bold')\n",
    "    \n",
    "    pivot_sig = heatmap_df.pivot(index='Configuration', columns='Metric', values='Significant')\n",
    "    for i in range(len(pivot_improvement.index)):\n",
    "        for j in range(len(pivot_improvement.columns)):\n",
    "            if pivot_sig.iloc[i, j]:\n",
    "                ax1.add_patch(Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=3))\n",
    "    \n",
    "    pivot_effect = heatmap_df.pivot(index='Configuration', columns='Metric', values='Effect_Size')\n",
    "    sns.heatmap(pivot_effect, annot=True, fmt='.2f', cmap='viridis',\n",
    "                ax=ax2, cbar_kws={'label': 'Effect Size (absolute)'})\n",
    "    ax2.set_title('Configuration Impact: Effect Sizes', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Metrics', fontweight='bold')\n",
    "    ax2.set_ylabel('')\n",
    "    \n",
    "    for i in range(len(pivot_effect.index)):\n",
    "        for j in range(len(pivot_effect.columns)):\n",
    "            if pivot_sig.iloc[i, j]:\n",
    "                ax2.add_patch(Rectangle((j, i), 1, 1, fill=False, edgecolor='white', lw=3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'rq2_configuration_effects_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    heatmap_df.to_csv(results_dir / 'rq2_all_configuration_effects.csv', index=False)\n",
    "    print(\"Configuration effects heatmap saved\")\n",
    "else:\n",
    "    print(\"Insufficient data for heatmap visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Utilization vs SCI Analysis\n",
    "\n",
    "Examining the relationship between system resource utilization and carbon intensity using dual-panel visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_data = []\n",
    "\n",
    "for exp_id, exp_df in experiment_data.items():\n",
    "    for _, row in exp_df.iterrows():\n",
    "        if pd.notna(row.get('vm_avg_cpu_percent')) and pd.notna(row.get('vm_total_io_read_count')):\n",
    "            resource_data.append({\n",
    "                'SCI_Score': row['sci_score'],\n",
    "                'CPU_Utilization': row['vm_avg_cpu_percent'],\n",
    "                'IO_Read_Operations': row['vm_total_io_read_count'] / 1e6,\n",
    "                'Experiment': exp_id,\n",
    "                'Category': experiments[exp_id]['category'],\n",
    "                'test_duration': row['test_duration']\n",
    "            })\n",
    "\n",
    "resource_df = pd.DataFrame(resource_data)\n",
    "\n",
    "if not resource_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    \n",
    "    scatter1 = ax1.scatter(\n",
    "        resource_df['CPU_Utilization'], resource_df['SCI_Score'],\n",
    "        c=resource_df['test_duration'], s=60, alpha=0.7,\n",
    "        cmap='plasma', edgecolors='black', linewidth=0.5\n",
    "    )\n",
    "    ax1.set_xlabel('CPU Utilization (%)', fontsize=12)\n",
    "    ax1.set_ylabel('SCI Score (gCO₂e/scenario)', fontsize=12)\n",
    "    ax1.set_title('SCI Score vs CPU Utilization', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    corr_cpu = resource_df['CPU_Utilization'].corr(resource_df['SCI_Score'])\n",
    "    ax1.text(0.05, 0.95, f'r = {corr_cpu:.3f}', transform=ax1.transAxes,\n",
    "             fontsize=12, fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    scatter2 = ax2.scatter(\n",
    "        resource_df['IO_Read_Operations'], resource_df['SCI_Score'],\n",
    "        c=resource_df['test_duration'], s=60, alpha=0.7,\n",
    "        cmap='plasma', edgecolors='black', linewidth=0.5\n",
    "    )\n",
    "    ax2.set_xlabel('I/O Read Operations (millions)', fontsize=12)\n",
    "    ax2.set_ylabel('SCI Score (gCO₂e/scenario)', fontsize=12)\n",
    "    ax2.set_title('SCI Score vs I/O Read Operations', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    corr_io = resource_df['IO_Read_Operations'].corr(resource_df['SCI_Score'])\n",
    "    ax2.text(0.05, 0.95, f'r = {corr_io:.3f}', transform=ax2.transAxes,\n",
    "             fontsize=12, fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])\n",
    "    cbar = fig.colorbar(scatter1, cax=cbar_ax)\n",
    "    cbar.set_label('Duration (s)', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.savefig(figures_dir / 'resource_utilization_vs_sci.png')\n",
    "    plt.show()\n",
    "    \n",
    "    correlation_results = pd.DataFrame({\n",
    "        'Metric_Pair': ['SCI_vs_CPU', 'SCI_vs_IO_Read', 'Duration_vs_CPU', 'Duration_vs_IO_Read'],\n",
    "        'Correlation': [\n",
    "            corr_cpu,\n",
    "            corr_io,\n",
    "            resource_df['test_duration'].corr(resource_df['CPU_Utilization']),\n",
    "            resource_df['test_duration'].corr(resource_df['IO_Read_Operations'])\n",
    "        ]\n",
    "    })\n",
    "    correlation_results.to_csv(results_dir / 'resource_correlations.csv', index=False)\n",
    "    \n",
    "    print(f\"Resource utilization plot saved to {figures_dir / 'resource_utilization_vs_sci.png'}\")\n",
    "    print(f\"Correlation analysis saved to {results_dir / 'resource_correlations.csv'}\")\n",
    "else:\n",
    "    print(\"Insufficient resource utilization data for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Partial Correlation Analysis\n",
    "\n",
    "Examining relationships between variables while controlling for potential confounders to identify true causal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_correlation(x, y, control_vars, data):\n",
    "    \"\"\"Calculate partial correlation coefficient controlling for specified variables\"\"\"\n",
    "    from scipy.stats import pearsonr\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    clean_data = data[[x, y] + control_vars].dropna()\n",
    "    \n",
    "    if len(clean_data) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    X_controls = clean_data[control_vars]\n",
    "    reg_x = LinearRegression().fit(X_controls, clean_data[x])\n",
    "    x_residuals = clean_data[x] - reg_x.predict(X_controls)\n",
    "    \n",
    "    reg_y = LinearRegression().fit(X_controls, clean_data[y])\n",
    "    y_residuals = clean_data[y] - reg_y.predict(X_controls)\n",
    "    \n",
    "    r, p_value = pearsonr(x_residuals, y_residuals)\n",
    "    \n",
    "    return r, p_value\n",
    "\n",
    "partial_correlations = []\n",
    "\n",
    "relationships = [\n",
    "    ('sci_score', 'powerjoular_power', ['test_duration']),\n",
    "    ('sci_score', 'powerjoular_util', ['test_duration']),\n",
    "    ('sci_score', 'vm_avg_memory_mb', ['test_duration', 'powerjoular_util']),\n",
    "    ('powerjoular_power', 'powerjoular_util', ['test_duration']),\n",
    "    ('test_duration', 'powerjoular_util', ['powerjoular_power'])\n",
    "]\n",
    "\n",
    "print(\"Partial Correlation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Relationship':<35} {'Zero-order r':<12} {'Partial r':<12} {'p-value':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for var1, var2, controls in relationships:\n",
    "    clean_data = all_data[[var1, var2]].dropna()\n",
    "    if len(clean_data) > 0:\n",
    "        zero_order_r, _ = pearsonr(clean_data[var1], clean_data[var2])\n",
    "        \n",
    "        partial_r, partial_p = partial_correlation(var1, var2, controls, all_data)\n",
    "        \n",
    "        if partial_r is not None:\n",
    "            relationship_name = f\"{var1} ↔ {var2}\"\n",
    "            controls_str = f\"(controlling for {', '.join(controls)})\"\n",
    "            \n",
    "            print(f\"{relationship_name:<35} {zero_order_r:<12.3f} {partial_r:<12.3f} {partial_p:<10.3f}\")\n",
    "            \n",
    "            partial_correlations.append({\n",
    "                'Variable_1': var1,\n",
    "                'Variable_2': var2,\n",
    "                'Control_Variables': ', '.join(controls),\n",
    "                'Zero_Order_r': zero_order_r,\n",
    "                'Partial_r': partial_r,\n",
    "                'P_Value': partial_p,\n",
    "                'Significant': partial_p < 0.05 if partial_p is not None else False\n",
    "            })\n",
    "\n",
    "if partial_correlations:\n",
    "    partial_corr_df = pd.DataFrame(partial_correlations)\n",
    "    partial_corr_df.to_csv(results_dir / 'partial_correlation_analysis.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nPartial correlation analysis saved to {results_dir / 'partial_correlation_analysis.csv'}\")\n",
    "    \n",
    "    print(\"\\nKey findings:\")\n",
    "    for _, row in partial_corr_df.iterrows():\n",
    "        diff = abs(row['Zero_Order_r'] - row['Partial_r'])\n",
    "        if diff > 0.2:\n",
    "            print(f\"  {row['Variable_1']} ↔ {row['Variable_2']}: \"\n",
    "                  f\"r changes from {row['Zero_Order_r']:.3f} to {row['Partial_r']:.3f} \"\n",
    "                  f\"when controlling for {row['Control_Variables']}\")\n",
    "else:\n",
    "    print(\"No partial correlation results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Research Question 3 Analysis\n",
    "\n",
    "### Carbon-Aware Configuration Effectiveness\n",
    "\n",
    "Evaluating the combined carbon-aware configuration (P3_E1) against the baseline to test H3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'P3_E1' in experiment_data:\n",
    "    carbon_optimized_data = experiment_data['P3_E1']\n",
    "    \n",
    "    baseline_sci = baseline_data['sci_score']\n",
    "    optimized_sci = carbon_optimized_data['sci_score']\n",
    "    \n",
    "    baseline_mean = baseline_sci.mean()\n",
    "    baseline_std = baseline_sci.std()\n",
    "    optimized_mean = optimized_sci.mean()\n",
    "    optimized_std = optimized_sci.std()\n",
    "    \n",
    "    improvement = ((baseline_mean - optimized_mean) / baseline_mean) * 100\n",
    "    \n",
    "    stat, p_value = mannwhitneyu(baseline_sci, optimized_sci, alternative='two-sided')\n",
    "    \n",
    "    pooled_std = np.sqrt(((len(baseline_sci)-1)*baseline_sci.var() + \n",
    "                         (len(optimized_sci)-1)*optimized_sci.var()) / \n",
    "                        (len(baseline_sci)+len(optimized_sci)-2))\n",
    "    cohens_d = (baseline_mean - optimized_mean) / pooled_std\n",
    "    \n",
    "    diff_mean = baseline_mean - optimized_mean\n",
    "    diff_std = np.sqrt(baseline_std**2/len(baseline_sci) + optimized_std**2/len(optimized_sci))\n",
    "    ci_lower = diff_mean - 1.96 * diff_std\n",
    "    ci_upper = diff_mean + 1.96 * diff_std\n",
    "    \n",
    "    print(\"RQ3 Analysis: Carbon-Aware Configuration Effectiveness\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Baseline (P1_E1): {baseline_mean:.3f} ± {baseline_std:.3f} gCO2e/scenario (n={len(baseline_sci)})\")\n",
    "    print(f\"Carbon-Optimized (P3_E1): {optimized_mean:.3f} ± {optimized_std:.3f} gCO2e/scenario (n={len(optimized_sci)})\")\n",
    "    print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "    print(f\"Absolute reduction: {diff_mean:.3f} gCO2e/scenario\")\n",
    "    print(f\"95% CI for difference: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "    print(f\"\\nStatistical significance: p = {p_value:.4f}\")\n",
    "    print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\nConclusion: H3.0 is REJECTED. Carbon-aware configuration significantly reduces SCI.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: H3.0 is NOT rejected. No significant reduction observed.\")\n",
    "    \n",
    "    rq3_results = pd.DataFrame({\n",
    "        'Metric': ['Baseline_Mean', 'Baseline_Std', 'Optimized_Mean', 'Optimized_Std', \n",
    "                  'Improvement_Percent', 'P_Value', 'Cohens_D', 'CI_Lower', 'CI_Upper'],\n",
    "        'Value': [baseline_mean, baseline_std, optimized_mean, optimized_std, \n",
    "                 improvement, p_value, cohens_d, ci_lower, ci_upper]\n",
    "    })\n",
    "    rq3_results.to_csv(results_dir / 'rq3_statistical_analysis.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"P3_E1 data not available for RQ3 analysis\")\n",
    "    improvement = None\n",
    "    p_value = None\n",
    "    cohens_d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary\n",
    "\n",
    "### Statistical Summary Tables\n",
    "\n",
    "Generating statistical summaries for inclusion in the thesis report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = []\n",
    "\n",
    "baseline_stats = {\n",
    "    'Configuration': 'P1_E1 Baseline',\n",
    "    'N': len(baseline_data),\n",
    "    'Mean_SCI': baseline_data['sci_score'].mean(),\n",
    "    'Std_SCI': baseline_data['sci_score'].std(),\n",
    "    'Median_SCI': baseline_data['sci_score'].median(),\n",
    "    'Min_SCI': baseline_data['sci_score'].min(),\n",
    "    'Max_SCI': baseline_data['sci_score'].max(),\n",
    "    'Mean_Power': baseline_data['powerjoular_power'].mean(),\n",
    "    'Mean_Duration': baseline_data['test_duration'].mean(),\n",
    "    'Success_Rate': baseline_data['test_success_rate'].mean()\n",
    "}\n",
    "summary_stats.append(baseline_stats)\n",
    "\n",
    "for exp_id in ['P2_E1', 'P2_E2', 'P2_E3', 'P2_E4', 'P2_E5']:\n",
    "    if exp_id in experiment_data:\n",
    "        exp_data = experiment_data[exp_id]\n",
    "        factor_col = experiments[exp_id].get('factor')\n",
    "        \n",
    "        if factor_col and factor_col in exp_data.columns:\n",
    "            for config in exp_data[factor_col].unique():\n",
    "                config_data_subset = exp_data[exp_data[factor_col] == config]\n",
    "                \n",
    "                config_stats = {\n",
    "                    'Configuration': f\"{exp_id}_{config}\",\n",
    "                    'N': len(config_data_subset),\n",
    "                    'Mean_SCI': config_data_subset['sci_score'].mean(),\n",
    "                    'Std_SCI': config_data_subset['sci_score'].std(),\n",
    "                    'Median_SCI': config_data_subset['sci_score'].median(),\n",
    "                    'Min_SCI': config_data_subset['sci_score'].min(),\n",
    "                    'Max_SCI': config_data_subset['sci_score'].max(),\n",
    "                    'Mean_Power': config_data_subset['powerjoular_power'].mean(),\n",
    "                    'Mean_Duration': config_data_subset['test_duration'].mean(),\n",
    "                    'Success_Rate': config_data_subset['test_success_rate'].mean()\n",
    "                }\n",
    "                summary_stats.append(config_stats)\n",
    "        else:\n",
    "            config_stats = {\n",
    "                'Configuration': f\"{exp_id}_no_compression\",\n",
    "                'N': len(exp_data),\n",
    "                'Mean_SCI': exp_data['sci_score'].mean(),\n",
    "                'Std_SCI': exp_data['sci_score'].std(),\n",
    "                'Median_SCI': exp_data['sci_score'].median(),\n",
    "                'Min_SCI': exp_data['sci_score'].min(),\n",
    "                'Max_SCI': exp_data['sci_score'].max(),\n",
    "                'Mean_Power': exp_data['powerjoular_power'].mean(),\n",
    "                'Mean_Duration': exp_data['test_duration'].mean(),\n",
    "                'Success_Rate': exp_data['test_success_rate'].mean()\n",
    "            }\n",
    "            summary_stats.append(config_stats)\n",
    "\n",
    "if 'P3_E1' in experiment_data:\n",
    "    p3_data = experiment_data['P3_E1']\n",
    "    p3_stats = {\n",
    "        'Configuration': 'P3_E1 Carbon-Optimized',\n",
    "        'N': len(p3_data),\n",
    "        'Mean_SCI': p3_data['sci_score'].mean(),\n",
    "        'Std_SCI': p3_data['sci_score'].std(),\n",
    "        'Median_SCI': p3_data['sci_score'].median(),\n",
    "        'Min_SCI': p3_data['sci_score'].min(),\n",
    "        'Max_SCI': p3_data['sci_score'].max(),\n",
    "        'Mean_Power': p3_data['powerjoular_power'].mean(),\n",
    "        'Mean_Duration': p3_data['test_duration'].mean(),\n",
    "        'Success_Rate': p3_data['test_success_rate'].mean()\n",
    "    }\n",
    "    summary_stats.append(p3_stats)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "if len(summary_df) > 0:\n",
    "    summary_df['Improvement_vs_Baseline'] = ((summary_df.iloc[0]['Mean_SCI'] - summary_df['Mean_SCI']) / \n",
    "                                             summary_df.iloc[0]['Mean_SCI']) * 100\n",
    "\n",
    "    summary_df.to_csv(results_dir / 'comprehensive_statistical_summary.csv', index=False)\n",
    "    \n",
    "    print(\"Top 5 Performing Configurations (by SCI reduction):\")\n",
    "    print(\"=\" * 55)\n",
    "    top_configs = summary_df.nlargest(5, 'Improvement_vs_Baseline')\n",
    "    for _, row in top_configs.iterrows():\n",
    "        print(f\"{row['Configuration']}: {row['Mean_SCI']:.3f} gCO2e/scenario ({row['Improvement_vs_Baseline']:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nComprehensive summary saved to {results_dir / 'comprehensive_statistical_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Hypothesis Testing Summary\n",
    "\n",
    "Providing definitive conclusions for both research questions based on statistical evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HYPOTHESIS TESTING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not rq2_df.empty:\n",
    "    significant_configs = rq2_df[rq2_df['significant'] & (rq2_df['improvement_percent'] > 0)]\n",
    "    print(f\"\\nRQ2 - Impact of Application-Level Configurations:\")\n",
    "    print(f\"Configurations tested: {len(rq2_df)}\")\n",
    "    print(f\"Significant improvements: {len(significant_configs)}\")\n",
    "    print(f\"Best improvement: {rq2_df['improvement_percent'].max():.1f}%\")\n",
    "    \n",
    "    if len(significant_configs) > 0:\n",
    "        print(\"\\nH2.0 REJECTED: At least one configuration significantly affects SCI score\")\n",
    "        print(\"H2.1 ACCEPTED: Configuration changes can significantly reduce carbon emissions\")\n",
    "        \n",
    "        print(\"\\nSignificant configurations:\")\n",
    "        for _, row in significant_configs.iterrows():\n",
    "            print(f\"  {row['experiment']}_{row['configuration']}: {row['improvement_percent']:+.1f}% (p={row['p_value']:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nH2.0 NOT REJECTED: No significant configuration effects detected\")\n",
    "        print(\"H2.1 REJECTED: Configuration changes do not significantly affect SCI score\")\n",
    "else:\n",
    "    print(\"\\nRQ2 - No comparison data available\")\n",
    "    significant_configs = pd.DataFrame()\n",
    "\n",
    "if 'P3_E1' in experiment_data and improvement is not None:\n",
    "    print(f\"\\n\\nRQ3 - Carbon-Aware Configuration Effectiveness:\")\n",
    "    print(f\"Improvement achieved: {improvement:.1f}%\")\n",
    "    print(f\"Statistical significance: p = {p_value:.4f}\")\n",
    "    print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05 and improvement > 0:\n",
    "        print(\"\\nH3.0 REJECTED: Combined configuration significantly reduces SCI score\")\n",
    "        print(\"H3.1 ACCEPTED: Carbon-aware optimization is effective\")\n",
    "        \n",
    "        if cohens_d >= 0.8:\n",
    "            effect_interpretation = \"large\"\n",
    "        elif cohens_d >= 0.5:\n",
    "            effect_interpretation = \"medium\"\n",
    "        elif cohens_d >= 0.2:\n",
    "            effect_interpretation = \"small\"\n",
    "        else:\n",
    "            effect_interpretation = \"negligible\"\n",
    "        \n",
    "        print(f\"Effect size interpretation: {effect_interpretation} practical significance\")\n",
    "    else:\n",
    "        print(\"\\nH3.0 NOT REJECTED: No significant improvement from combined configuration\")\n",
    "        print(\"H3.1 REJECTED: Carbon-aware optimization not effective\")\n",
    "else:\n",
    "    print(\"\\n\\nRQ3 - Carbon-aware configuration data not available\")\n",
    "\n",
    "hypothesis_results = {\n",
    "    'RQ2_H20_Status': 'Rejected' if not rq2_df.empty and len(significant_configs) > 0 else 'Not Rejected',\n",
    "    'RQ2_H21_Status': 'Accepted' if not rq2_df.empty and len(significant_configs) > 0 else 'Rejected',\n",
    "    'RQ2_Significant_Configs': len(significant_configs) if not rq2_df.empty else 0,\n",
    "    'RQ2_Best_Improvement': rq2_df['improvement_percent'].max() if not rq2_df.empty else None,\n",
    "    'RQ3_H30_Status': 'Rejected' if (improvement is not None and p_value < 0.05 and improvement > 0) else 'Not Rejected',\n",
    "    'RQ3_H31_Status': 'Accepted' if (improvement is not None and p_value < 0.05 and improvement > 0) else 'Rejected',\n",
    "    'RQ3_Improvement_Percent': improvement,\n",
    "    'RQ3_P_Value': p_value,\n",
    "    'RQ3_Effect_Size': cohens_d\n",
    "}\n",
    "\n",
    "hypothesis_df = pd.DataFrame([hypothesis_results])\n",
    "hypothesis_df.to_csv(results_dir / 'hypothesis_testing_results.csv', index=False)\n",
    "\n",
    "print(f\"\\nHypothesis testing results saved to {results_dir / 'hypothesis_testing_results.csv'}\")\n",
    "print(f\"\\nAnalysis complete. All figures saved to {figures_dir}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for csv_file in results_dir.glob('*.csv'):\n",
    "    print(f\"- {csv_file.name}\")\n",
    "print(\"\\nGenerated figures:\")\n",
    "for png_file in figures_dir.glob('*.png'):\n",
    "    print(f\"- {png_file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
